# Domande e Risposte - Calcolatori Elettronici A.A. 2024-2025

## Indice
- [Domande e Risposte - Calcolatori Elettronici A.A. 2024-2025](#domande-e-risposte---calcolatori-elettronici-aa-2024-2025)
	- [Indice](#indice)
	- [1. Architettura di Base e Memoria](#1-architettura-di-base-e-memoria)
		- [Domanda 1.1 (answered)](#domanda-11-answered)
		- [Domanda 1.2 (answered)](#domanda-12-answered)
		- [Domanda 1.3 (answered)](#domanda-13-answered)
		- [Domanda 1.4 (answered)](#domanda-14-answered)
	- [2. Memoria Cache](#2-memoria-cache)
		- [Domanda 2.1 (answered)](#domanda-21-answered)
		- [Domanda 2.2 (answered)](#domanda-22-answered)
		- [Domanda 2.3 (answered)](#domanda-23-answered)
		- [Domanda 2.4 (answered)](#domanda-24-answered)
		- [Domanda 2.5](#domanda-25)
		- [Domanda 2.6 (answered)](#domanda-26-answered)
		- [Domanda 2.7](#domanda-27)
		- [Domanda 2.8](#domanda-28)
	- [3. Interruzioni](#3-interruzioni)
		- [Domanda 3.1 (answered)](#domanda-31-answered)
		- [Domanda 3.2 (answered)](#domanda-32-answered)
		- [Domanda 3.3 (answered)](#domanda-33-answered)
		- [Domanda 3.4 (answered)](#domanda-34-answered)
		- [Domanda 3.5](#domanda-35)
		- [Domanda 3.6](#domanda-36)
		- [Domanda 3.7](#domanda-37)
		- [Domanda 3.8](#domanda-38)
		- [Domanda 3.9](#domanda-39)
		- [Domanda 3.10](#domanda-310)
		- [Domanda 3.11](#domanda-311)
		- [Domanda 3.12](#domanda-312)
		- [Domanda 3.13](#domanda-313)
	- [4. Eccezioni](#4-eccezioni)
		- [Domanda 4.1 (answered)](#domanda-41-answered)
		- [Domanda 4.2 (answered)](#domanda-42-answered)
		- [Domanda 4.3](#domanda-43)
		- [Domanda 4.4](#domanda-44)
	- [5. Protezione](#5-protezione)
		- [Domanda 5.1 (answered)](#domanda-51-answered)
		- [Domanda 5.2](#domanda-52)
		- [Domanda 5.3 (answered)](#domanda-53-answered)
		- [Domanda 5.4](#domanda-54)
		- [Domanda 5.5](#domanda-55)
		- [Domanda 5.6](#domanda-56)
		- [Domanda 5.7](#domanda-57)
	- [6. Paginazione e Memoria Virtuale](#6-paginazione-e-memoria-virtuale)
		- [Domanda 6.1](#domanda-61)
		- [Domanda 6.2 (answered)](#domanda-62-answered)
		- [Domanda 6.3](#domanda-63)
		- [Domanda 6.4](#domanda-64)
		- [Domanda 6.5](#domanda-65)
		- [Domanda 6.6](#domanda-66)
		- [Domanda 6.7](#domanda-67)
		- [Domanda 6.8](#domanda-68)
		- [Domanda 6.9](#domanda-69)
		- [Domanda 6.10](#domanda-610)
		- [Domanda 6.11](#domanda-611)
		- [Domanda 6.12](#domanda-612)
		- [Domanda 6.13](#domanda-613)
		- [Domanda 6.14](#domanda-614)
		- [Domanda 6.15](#domanda-615)
		- [Domanda 6.16](#domanda-616)
		- [Domanda 6.17](#domanda-617)
		- [Domanda 6.18](#domanda-618)
		- [Domanda 6.19](#domanda-619)
		- [Domanda 6.20](#domanda-620)
	- [7. Sistemi Multiprocesso e Processi](#7-sistemi-multiprocesso-e-processi)
		- [Domanda 7.1 (answered)](#domanda-71-answered)
		- [Domanda 7.2](#domanda-72)
		- [Domanda 7.3](#domanda-73)
		- [Domanda 7.4](#domanda-74)
		- [Domanda 7.5](#domanda-75)
		- [Domanda 7.6](#domanda-76)
		- [Domanda 7.7](#domanda-77)
		- [Domanda 7.8](#domanda-78)
		- [Domanda 7.9](#domanda-79)
		- [Domanda 7.10](#domanda-710)
		- [Domanda 7.11](#domanda-711)
	- [8. Realizzazione delle Primitive](#8-realizzazione-delle-primitive)
		- [Domanda 8.1 (answered)](#domanda-81-answered)
		- [Domanda 8.2](#domanda-82)
		- [Domanda 8.3](#domanda-83)
		- [Domanda 8.4](#domanda-84)
		- [Domanda 8.5](#domanda-85)
		- [Domanda 8.6](#domanda-86)
		- [Domanda 8.7](#domanda-87)
		- [Domanda 8.8](#domanda-88)
		- [Domanda 8.9](#domanda-89)
		- [Domanda 8.10](#domanda-810)
	- [9. Semafori](#9-semafori)
		- [Domanda 9.1 (answered)](#domanda-91-answered)
		- [Domanda 9.2](#domanda-92)
		- [Domanda 9.3](#domanda-93)
	- [10. Delay e Gestione del Tempo](#10-delay-e-gestione-del-tempo)
		- [Domanda 10.1 (answered)](#domanda-101-answered)
		- [Domanda 10.2](#domanda-102)
		- [Domanda 10.3](#domanda-103)
	- [11. Bus PCI](#11-bus-pci)
		- [Domanda 11.1 (answered)](#domanda-111-answered)
		- [Domanda 11.2](#domanda-112)
	- [12. I/O e Driver](#12-io-e-driver)
		- [Domanda 12.1 (answered)](#domanda-121-answered)
		- [Domanda 12.2](#domanda-122)
		- [Domanda 12.3](#domanda-123)
		- [Domanda 12.4](#domanda-124)
		- [Domanda 12.5](#domanda-125)
		- [Domanda 12.6](#domanda-126)
		- [Domanda 12.7](#domanda-127)
		- [Domanda 12.8](#domanda-128)
		- [Domanda 12.9](#domanda-129)
		- [Domanda 12.10](#domanda-1210)
		- [Domanda 12.11](#domanda-1211)
	- [13. DMA (Direct Memory Access)](#13-dma-direct-memory-access)
		- [Domanda 13.1 (answered)](#domanda-131-answered)
		- [Domanda 13.2](#domanda-132)
		- [Domanda 13.3](#domanda-133)
		- [Domanda 13.4](#domanda-134)
		- [Domanda 13.5](#domanda-135)
		- [Domanda 13.6](#domanda-136)
		- [Domanda 13.7](#domanda-137)
		- [Domanda 13.8](#domanda-138)
		- [Domanda 13.9](#domanda-139)
		- [Domanda 13.10 (answered)](#domanda-1310-answered)
		- [Domanda 13.11](#domanda-1311)
	- [14. Architettura Moderna CPU](#14-architettura-moderna-cpu)
		- [Domanda 14.1 (answered)](#domanda-141-answered)
		- [Domanda 14.2](#domanda-142)
		- [Domanda 14.3](#domanda-143)
		- [Domanda 14.4](#domanda-144)
		- [Domanda 14.5](#domanda-145)
		- [Domanda 14.6](#domanda-146)
		- [Domanda 14.7](#domanda-147)
		- [Domanda 14.8](#domanda-148)
	- [Note per lo Studio](#note-per-lo-studio)

<div class="stop"></div>

## 1. Architettura di Base e Memoria

### Domanda 1.1 (answered)
**Domanda:** Nell'architettura CPU-Memoria-I/O che stiamo studiando, chi conosce cosa? Spieghi dettagliatamente la conoscenza e ignoranza di ciascun componente del sistema.

**Risposta:**
Nell'architettura classica CPU-Memoria-I/O, ogni componente ha una visione parziale del sistema, fondamentale per la separazione dei compiti e la sicurezza.

**CPU:**
> La CPU conosce solo i propri registri interni e può accedere alla memoria tramite indirizzi, ma non ha conoscenza diretta della struttura fisica della RAM o delle periferiche. La CPU invia segnali di lettura/scrittura e indirizzi, ma non "vede" cosa c'è oltre l'interfaccia di memoria.

**Memoria (RAM):**
> La memoria è un dispositivo passivo: non conosce la CPU né le periferiche. Si limita a rispondere alle richieste di lettura/scrittura sugli indirizzi che le vengono forniti. Non ha consapevolezza di chi stia accedendo o del significato dei dati.

**Periferiche/I/O:**
> Le periferiche sono progettate per rispondere a comandi specifici, ma non conoscono la CPU o la RAM. Interagiscono tramite registri di controllo e dati, e possono generare segnali di interruzione per notificare eventi alla CPU.

**Riferimento:**
> "La CPU comunica con la memoria e le periferiche tramite bus dedicati. Ogni componente è progettato per ignorare i dettagli interni degli altri, garantendo modularità e sicurezza."
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#2-architettura-cpu-memoria-io)*

**Esempio pratico:**
> Quando la CPU esegue un'istruzione di scrittura, invia un indirizzo e un dato sul bus. La RAM riceve la richiesta e memorizza il dato, senza sapere se proviene da un programma utente o dal sistema operativo. Analogamente, una periferica riceve comandi senza sapere chi li ha generati.

**Conclusione:**
Questa architettura a conoscenza limitata permette di realizzare sistemi scalabili, sicuri e facilmente espandibili.

---

### Domanda 1.2 (answered)
**Domanda:** Come viene gestito il flusso di controllo al momento dell'avvio del calcolatore? Cosa succede quando il contenuto della RAM è casuale?

**Risposta:**
Il flusso di controllo all'avvio del calcolatore è gestito attraverso una sequenza ben definita di passaggi che iniziano dalla ROM e portano all'esecuzione del sistema operativo, gestendo il problema critico del contenuto casuale della RAM.

**Il problema del contenuto casuale della RAM:**

**Problema fondamentale all'avvio:**
> Al'avvio del calcolatore però il contenuto della memoria **RAM** è _casuale_.
> È quindi necessario che il programma di `bootstrap`, ovvero quelle informazioni necessarie a inizializzare in maniera corretta l'`%rip`.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#21-flusso-di-controllo)*

**Soluzione tramite ROM e BIOS:**
> Il programma `bootstrap` deve essere salvato in una memoria **ROM** sulla quale il programma deve essere salvato.
>
> All'inizio della storia dei calcolatori il programma di `bootstrap` veniva caricato **manualmente** dagli operatori ad ogni accensione del calcolatore.
> La **ROM** permette di non dover fare più questa operazione, è infatti sufficente impostare che in fase di `/reset` la **CPU** vada a consultare quest'ultima. Il `BIOS` è contenuto in questa parte di programma.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#21-flusso-di-controllo)*

**Sequenza di avvio nei processori Intel:**

**Modalità di avvio progressive:**
> I processori _intel_ sono ancora oggi progettati per avviarsi a `16bit` in modalità non protetta.
> Via _software_ vengono poi portati in modalità protetta a `32bit`, in generale grazie ad un programma di `bootstrap` nel `BIOS`.
> Nel nostro caso sarà l'emulatore stesso ad effettuare questo primo passaggio.
>
> Tocca a noi però portare il processore nella modalità a `64bit`, e questo compito lo facciamo svolgere al programma `boot.bin` che può cedere il controllo al modulo `sistema`.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#242-avvio-sistema)*

**Processo di bootstrap dettagliato:**

**Il bootloader e il caricamento dei moduli:**
> Il programma `boot.bin` esegue una serie di allocazioni in memoria per permettere il corretto funzionamento della nostra macchina.
>
> Le righe 1-15 arrivano dal programma `boot.bin`:
> - Nelle righe 4–7 il programma `boot.bin` ci informa del fatto che il `bootloader` precedente (nel nostro caso `QEMU` stesso) ha caricato in memoria _tre file_, in particolare il file `build/sistema.strip4` all'indirizzo `0x114000`.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#242-avvio-sistema)*

**Esempio concreto della sequenza di avvio:**
Dal log di avvio del sistema possiamo vedere la sequenza completa:

> ```log
> 1 | [INF] - Boot loader di Calcolatori Elettronici, v1.0
> 2 | [INF] - Memoria totale: 32 MiB, heap: 636 KiB
> 3 | [INF] - Argomenti: /home/gabrieledc/CE/lib/ce/boot.bin
> 4 | [INF] - Il boot loader precedente ha caricato 3 moduli:
> 5 | [INF] - - mod[0]: start=114000 end=12f580 file=build/sistema.strip
> 6 | [INF] - - mod[1]: start=130000 end=1414e0 file=build/io.strip
> 7 | [INF] - - mod[2]: start=142000 end=147400 file=build/utente.strip
> 8 | [INF] - Copio mod[0] agli indirizzi specificati nel file ELF:
> 9 | [INF] - - copiati 108560 byte da 114000 a 200000
> 10 | [INF] - - copiati 970 byte da 12edb8 a 21bdb8
> 11 | [INF] - - azzerati ulteriori 79030 byte
> 12 | [INF] - - entry point 200178
> 13 | [INF] - Creata finestra sulla memoria centrale: [ 1000, 2000000)
> 14 | [INF] - Creata finestra per memory-mapped-IO: [ 2000000, 100000000)
> 15 | [INF] - Attivo la modalita' a 64 bit e cedo il controllo a mod[0]...
> ```
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#242-avvio-sistema)*

**Controllo del flusso di esecuzione:**

**Principio fondamentale:**
> Abbiamo più volte detto che la **CPU** non fa altro che leggere e eseguire le istruzioni una alla volta.
> Per capire però dove recuperare una nuova istruzione una volta eseguita quella precedente va a consultare l'_istruction Pointer_ `%rip` che viene aggiornato dell'operazione precedente.
>
> Il controllo **è unico** e può essere solo scambiato tra i vari stati della **CPU**, a loro volta scanditi dal _programma in esecuzione_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#21-flusso-di-controllo)*

**Attivazione della modalità 64-bit:**
Il passaggio finale alla modalità a 64-bit avviene attraverso la paginazione:

> ```x86asm
> ; settiamo il bit 31 di CR0
>     MOVl %cr0, %eax
>     ORl $0x80010000, %eax    // paging & write-protect
>     MOVl %eax, %cr0
> ; ora la modalità a 64 bit è attiva ...
> ```
>
> *Fonte: [Memoria Virtuale nel Nucleo.md](./Memoria%20Virtuale%20nel%20Nucleo)*

**Gestione della memoria durante l'avvio:**

**Inizializzazione delle traduzioni di memoria:**
> Le traduzioni della parte `sistema/condivisa` sono create dal _bootloader_ **prima di abilitare la paginazione** tramite la funzione `crea_finestra_FM()`.
>
> La modifica al _bootstrap_ di un processo per creare questa opzione è in realtà abbastanza banale, in quanto all'accensione la `MMU` è disattivata, e la **CPU** utilizza direttamente gli indirizzi **fisici**.
>
> *Fonte: [Memoria Virtuale nel Nucleo.md](./Memoria%20Virtuale%20nel%20Nucleo) e [Paginazione.md](./Paginazione#331-traduzioni-identità)*

**Inizializzazione del sistema:**
Una volta completato il bootstrap, il sistema inizializza i suoi componenti:

> - Nelle righe 36-37 vengono creati i primi processi di sistema
> - Alla riga 38 vediamo che viene inizializzato l'`APIC`.
> - Nella riga 40 veniamo informati dell'inizializzazione dello _heap di sistema_ (riutilizzando lo spazio occupato da `boot.bin`).
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#242-avvio-sistema)*

In sintesi, il flusso di controllo all'avvio è gestito attraverso una catena di responsabilità: ROM/BIOS → bootloader → boot.bin → modulo sistema, dove ogni componente ha il compito di inizializzare il successivo e trasferirgli il controllo, risolvendo il problema del contenuto casuale della RAM attraverso l'uso di memoria non volatile (ROM) per i programmi di bootstrap.

---

### Domanda 1.3 (answered)
**Domanda:** Nell'architettura CPU-Memoria-I/O, descriva dettagliatamente come avviene la comunicazione tra CPU e RAM. Cosa sono i segnali `/be` (Byte Enabler) e come vengono utilizzati nelle operazioni di scrittura?

**Risposta:**
La comunicazione tra CPU e RAM nell'architettura CPU-Memoria-I/O avviene attraverso un sistema di bus strutturato con specifici segnali di controllo, dove i Byte Enabler (`/be`) svolgono un ruolo cruciale per permettere operazioni selettive sui singoli byte.

**Architettura generale CPU-Memoria-I/O:**

**Struttura fondamentale:**
> Andremo a studiare un'architettura Architettura CPU - Memoria (RAM e ROM) - I/O.
> Questa architettura è stata progettata con lo scopo di **eseguire software**
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#2-architettura-cpu-memoria-io)*

**Struttura del bus di comunicazione:**

**Composizione del bus CPU-RAM:**
> **CPU** e **RAM** sono collegate da un _bus_ che è composto da:
> - `D`: Rappresenta le **Linee di indirizzo**. In teoria sarebbero `64` ma nei processori _Intelx86_ il massimo è `57`. Nei processori comuni tendenzialmente questi fili però sono "solamente" `48`. Questo perché $2^{64}$ indirizzi di memoria sono decisamente troppi per un calcolatore comune.
> - `A`: indica il **numero di riga** in **RAM** alla quale vogliamo accedere.
> - `C`: rappresenta i fili che contengolo le variabili di controllo (`/mw`, `/mr`, ...)
> - `/be`: sono i _Byte Enabler_. Ne esiste uno per ogni byte di _una riga_. Vengono utilizzati principalmente in scrittura, perché permettono di poter modificare solamente i singoli bit in un indirizzo.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Interfaccia dei chip di memoria:**

**Collegamenti del singolo chip di RAM:**
> Il singolo chip di **RAM** avrà come collegamenti possibili:
> - `/s`: select
> - `A`: indirizzo
> - `/r`: comando di lettura
> - `/w`: comando di scrittura
> - `D`: dati, su `8bit`
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**I segnali Byte Enabler (`/be`):**

**Definizione e funzione:**
> Ciò significa che dato un indirizzo `A[63:0]`, il numero di riga verrà identificato dai bit `A[63:3]`. I restanti `A[2:0]` rappresentano l'offset all'interno della riga, e vengono chiamati **_Byte Enabler_** `BE`.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche)*

**Utilizzo nelle operazioni di scrittura:**
I Byte Enabler permettono di effettuare operazioni selettive sui singoli byte di una riga di memoria, evitando di dover riscrivere l'intera riga quando si vuole modificare solo una parte.

**Implementazione hardware dei Byte Enabler:**

**Collegamento dei chip di memoria:**
Nell'implementazione hardware, ogni chip di memoria è collegato attraverso un sistema che utilizza i Byte Enabler per determinare quale parte della riga deve essere effettivamente scritta:

> ```verilog
> assign w_ = W_;
> assign r_ = R_;
>
> assign Di = D[8*i + 7 : 8*i];
> // Il padding corretto del bus dei dati
>
> assign Ai = A[(k-3)-1 : 0];
> /*
> * k-3 perché ogni riga di indirizzo contiene 8 indirizzi,
> * Se tutti i chip devono memorizzare 2^k byte, ognuno ne memorizzerà:
> *   2^k / 8 -> 2^(k-3) byte
> * Ciò implica che
> */
> wire mask_; assign mask_ = A[n - 1 : n - k] ^ indirizzo_di_riga;
> /*
> * Metto in OR (sono attivi bassi) :
> *  - Il bit corrispettivo del Byte Enabler
> *  - XOR tra la parte alta di A e l'indirizzo della riga
> *
> * Questo si fa per determinare se la regione è quella interpellata
> */
> assign s_ = (BE_[i] | mask_);
> ```
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Esempio di utilizzo nel bus PCI:**

**Byte Enabler nel contesto PCI:**
I Byte Enabler sono utilizzati anche in altri contesti sistemici, come nel bus PCI:

> | `BE#[3:0]` | uscita | ingresso | Fungono da _byte-enabler_ nelle fasi di trasferimento (`BE#[3:0]`) |
>
> *Fonte: [PCI.md](./PCI)*

**Caratteristiche delle operazioni di memoria:**

**Formato delle istruzioni Intel x86:**
> Il formato delle istruzioni _Intelx86_ può avere al massimo **1 operando esplicito in memoria**.
>
> Tuttavia è possibile operare con due operatori in memoria, attraverso **operazioni che hanno accessi impliciti**.
> Alcuni esempi sono la `MOVS`, `PUSH (%rdi)`, `POP(%rdi)`.
>
> Una qualsiasi informazione che va nella memoria possiede due proprietà:
> - **Indirizzo**: indirizzo della prima locazione di memoria desiderata
> - **Dimensione**: talvolta si deduce dai registri, talvolta necessita di essere esplicitata con il _suffisso_, indica a quanti byte vogliamo accedere.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Dimensioni degli accessi in memoria:**

**Tipi di accesso supportati:**
> Nelle memorie per processori _Intelx86_ che vedremo le dimensioni degli accessi in memoria sono i seguenti:
>
> | | `B` | `W` | `L` | `Q` |
> |:---:|:---:|:---:|:---:|:---:|
> | Byte | `1` | `2` | `4` | `8` |
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#23-memoria)*

**Importanza degli allineamenti:**

**Necessità di accessi multipli per dati non allineati:**
> Data questa configurazione risulta chiara l'importanza degli allineamenti.
>
> Se volessimo infatti accedere a indirizzi non allineati è **necessario** fare **2 accessi**.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

In sintesi, la comunicazione CPU-RAM avviene attraverso un bus strutturato con linee dati, indirizzi e controllo, dove i segnali `/be` (Byte Enabler) permettono operazioni selettive sui singoli byte, ottimizzando le prestazioni ed evitando riscritture non necessarie di intere righe di memoria. Questo meccanismo è fondamentale per l'efficienza del sistema di memoria e viene implementato a livello hardware attraverso logiche di selezione che determinano quali chip di memoria devono essere attivati per ciascuna operazione.

---

### Domanda 1.4 (answered)
**Domanda:** Spieghi il problema degli accessi non allineati alla memoria. Come viene gestito dall'hardware il caso di un'operazione `MOVQ 4097, %RAX` e quale ruolo ha il μ-codice della CPU?

**Risposta:**
Gli accessi non allineati alla memoria rappresentano un problema fondamentale nell'architettura dei calcolatori perché violano il principio di allineamento naturale degli oggetti in memoria.

**Allineamento naturale degli oggetti:**
Un oggetto si dice allineato naturalmente se il suo indirizzo è divisibile per la sua dimensione. Formalmente:
> Un oggetto $o$ si dice _allineato naturalmente_ se
> $$
>     \boxed{|o|_{sizeof(o)} = 0}
> $$
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#243-allineamento-naturale)*

**Tipi di allineamento richiesti:**
- Un `BYTE` (1 byte) può essere posizionato a qualsiasi indirizzo
- Un `WORD` (2 bytes) deve essere allineato a indirizzi pari (divisibili per 2)
- Un `LONG` (4 bytes) deve essere allineato a indirizzi divisibili per 4
- Un `QUAD` (8 bytes) deve essere allineato a indirizzi divisibili per 8

**Organizzazione della memoria RAM:**
> Nella memoria RAM possiamo identificare una regione come uno spazio di `8Byte`.
> Assegneremo quindi ad ognuna un numero, detto **numero di riga**, che la identificherà.
>
> Ciò significa che dato un indirizzo `A[63:0]`, il numero di riga verrà identificato dai bit `A[63:3]`. I restanti `A[2:0]` rappresentano l'offset all'interno della riga, e vengono chiamati **_Byte Enabler_** `BE`.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#25-regione-e-confine)*

**Analisi del caso specifico: `MOVQ 4097, %RAX`:**
L'operazione `MOVQ 4097, %RAX` richiede la lettura di 8 byte consecutivi a partire dall'indirizzo 4097.

**Scomposizione dell'indirizzo 4097 (0x1001):**
- Numero di riga: `A[63:3]` = 512 (0x200)
- Offset nella riga: `A[2:0]` = 1

**Il problema dell'accesso non allineato:**
Un `QUAD` (8 bytes) che inizia all'offset 1 si estende fino all'offset 8, ma ogni riga contiene solo gli offset da 0 a 7. Questo significa che i dati richiesti si trovano **a cavallo di due righe consecutive**.

**Gestione hardware:**
> Data questa configurazione risulta chiara l'importanza degli allineamenti.
> Se volessimo infatti accedere a indirizzi non allineati è **necessario** fare **2 accessi**.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Processo di gestione hardware:**
1. **Primo accesso:** Legge la riga 512 per ottenere i byte agli offset 1, 2, 3, 4, 5, 6, 7 (7 bytes)
2. **Secondo accesso:** Legge la riga 513 per ottenere il byte all'offset 0 (1 byte rimanente)

**Ricomposizione dei dati:**
> Inoltre potrebbe diventare necessario sistemare il padding dei byte, in quanto quelli all'indirizzo precedente si trovano nella regione delle _MSB_, anche se per quello che ci riguarda sono nella _LSB_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Il ruolo del μ-codice della CPU:**
> Tutte queste operazioni vengono eseguite non dal software (l'operazione `MOVQ 4097, %RAX` di fatto fa tutto in una riga), ma dall'**hardware**, in particolare nel _$\mu$-codice che implementa l'accesso in memoria_ che si trova nella **CPU**.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#26-comunicazione-cpu-memoria)*

**Operazioni gestite dal μ-codice:**
1. **Rilevare** che l'accesso è non allineato
2. **Scomporre** l'operazione singola in due accessi separati
3. **Gestire** i due accessi alla memoria consecutivi
4. **Ricomporre** i dati letti nelle posizioni corrette
5. **Presentare** il risultato finale come se fosse stato un singolo accesso

**Impatto sulle prestazioni:**
Gli accessi non allineati hanno un costo significativo perché:
- Richiedono **doppi accessi** alla memoria
- Aumentano il **traffico sul bus**
- Possono causare **miss aggiuntivi** nella cache
- Richiedono **elaborazione extra** da parte del μ-codice

Per questo motivo, i compilatori cercano sempre di allineare naturalmente gli oggetti in memoria, e i programmatori devono prestare attenzione all'allineamento delle strutture dati per ottenere prestazioni ottimali.

---
<div class="stop"></div>

---

## 2. Memoria Cache

### Domanda 2.1 (answered)
**Domanda:** A che serve la Cache e come funziona? Descriva i principi di funzionamento fondamentali.

**Risposta:**
La memoria cache è una soluzione hardware fondamentale per risolvere il problema della disparità di velocità tra processore e memoria RAM.

**Problema che risolve la cache:**
> La **RAM** è estrememante lenta rispetto al processore, circa 200/300 volte più lenta, ciò mette in attesa il processore.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Soluzione technologica:**
> Un modo per avere **RAM** più veloci è quello di utilizzare le **RAM Statiche** invece di quelle _dinamiche_.
> Le **RAM Statiche** conservano l'informazione tramite `Flip-Flop`, e sono realizzabili con 6/7 transistor.
> Le **RAM Dinamiche** invece utilizzano microcondensatori che necessitano che l'informazione venga periodicamente "rinfrescata".

| RAM Dinamiche | RAM Statiche |
| :-----------: | :----------: |
|    Grandi     |   Piccole    |
|  Economiche   |   Costose    |
|     Lente     |    Veloci    |

> Esiste tuttavia un modo per poter utilizzare **RAM** _grandi, economiche e veloci_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Principi di località:**
La cache si basa sui **principi di località**, che sono osservazioni statistiche sul comportamento dei programmi:

> Il codice infatti si distribuisce in locazioni di memoria sequenziali, e, statisticamente, **raramente** effettua salti casuali tra istruzioni.
> Su questa assunzione di base si fondano i due **_Principi di Località_**:
>
> **Principio di località Temporale**: visto un dato è probabile che molto presto si voglia utilizzare di nuovo.
>
> **Principio di località Spaziale**: visto un indirizzo è probabile che a breve ci si ritorni.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Funzionamento del controllore cache:**
> La memoria _cache_ funziona proprio basandosi su questi principi.
> Infatti quando eseguiamo diversi accessi alla **RAM**, la _cache_ sarà in ascolto di letture e scritture, salvando in locale i dati che i principi dicono che serviranno.
>
> L'esecuzione della _cache_ è gestita da un **_controllore_** che lavora in maniera totalmente trasparente, **senza che il processore e il programmatore sappiano della sua esistenza**.
> Tuttavia il programmatore può _approfittare dei principi_, affinché possa sfruttare al massimo la _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Meccanismo di funzionamento:**
> Il controllore verifica che un dato richiesto dalla CPU sia già stato memorizzato.
> Se lo è stato lo invia immediatamente, altrimenti effettua una lettura in **RAM** e lo invia alla **CPU**.
> Prima di inviarlo però lo salva localmente, eventualmente rimpiazzando altri dati che erano già salvati.
> La scelta di quale dato sovrascrivere può essere determinata automaticamente dall'architettura (come nel nostro caso) oppure può utilizzare diversi meccanismi di selezione specifici dell'architettura stessa.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Hit e Miss:**
> Quando il processore richiede una locazione di memoria, si effettua un controllo per verificare che si trovi o meno nella _cache_.
> Il segnale di `hit` indica che la memoria si trova già nella _cache_, perciò è sufficente leggere quella.
>
> Il segnale di `miss` indica invece che la memoria non è nella _cache_, perciò va recuperato dalla **RAM** per poter essere letto.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Politiche di scrittura:**
In caso di scrittura con `hit` abbiamo due possibili politiche:
- **`Write Through`**: scrive il nuovo valore sia in _cache_ che in **RAM**
- **`Write Back`**: scrive il nuovo valore solo nella _cache_

Anche in caso di `miss` la scrittura ha due possibili politiche:
- **`Write Allocate`**: copia l'elemento dalla **RAM** nella _cache_ prima della modifica, e poi lo ritrasmette aggiornato
- **`Write No-Allocate`**: effettua l'aggiornamento **solo** in **RAM**, senza salvare nulla in _cache_

**Ottimizzazione per località spaziale:**
> Il motivo per il quale, data una riga, recuperiamo in cache tutta la sezione dov'è contenuta è perché, per il principio di località, è probabile che il processore richieda in un secondo momento locazioni vicine (_località spaziale_).
> Inoltre, se il tempo di lettura di una riga fosse $t$, quella di lettura di un blocco è un tempo $\ll 8t$.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Trasparenza:**
> La memoria cache lavora _solo_ sulla **RAM**, non ha alcun senso che lavori per l'_I/O_, poiché manca il principio base (_I/O_ ha effetti collaterali voluti).
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

In sintesi, la cache è una memoria piccola, veloce e costosa che funge da buffer intelligente tra CPU e RAM, sfruttando i principi di località per predire quali dati serviranno al processore, migliorando drasticamente le prestazioni del sistema.

---

### Domanda 2.2 (answered)
**Domanda:** I principi di località valgono sempre? Quando sono rispettati i principi di località?

**Risposta:**

I principi di località **non valgono sempre** e sono violati in specifici pattern di accesso alla memoria. La loro validità dipende dal tipo di algoritmo e dalla struttura dati utilizzata.

**Definizione dei principi di località:**

**Principi fondamentali:**
> Il codice infatti si distribuisce in locazioni di memoria sequenziali, e, statisticamente, **raramente** effettua salti casuali tra istruzioni.
> Su questa assunzione di base si fondano i due **_Principi di Località_**:
>
> **Principio di località Temporale**: visto un dato è probabile che molto presto si voglia utilizzare di nuovo.
>
> **Principio di località Spaziale**: visto un indirizzo è probabile che a breve ci si ritorni.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Presupposto statistico:**
> Infatti, nonostante l'accesso del programmatore alla memoria sia per definizione _casuale_, ovvero non predeterminato, in realtà nella maggior parte dei casi non lo è realmente.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Quando i principi di località sono rispettati:**

**1. Codice sequenziale:**
Il principio di località spaziale è rispettato quando il codice viene eseguito in sequenza, senza salti o chiamate frequenti a funzioni distanti:

> Il codice infatti si distribuisce in locazioni di memoria sequenziali, e, statisticamente, **raramente** effettua salti casuali tra istruzioni.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**2. Accessi ad array contigui:**
Gli accessi sequenziali a array rispettano entrambi i principi:
- **Località spaziale**: elementi consecutivi sono in locazioni di memoria adiacenti
- **Località temporale**: spesso si accede agli stessi elementi più volte

**3. Strutture dati con referenza spaziale:**
Oggetti e strutture allocate consecutivamente beneficiano della località spaziale:

> Il motivo per il quale, data una riga, recuperiamo in cache tutta la sezione dov'è contenuta è perché, per il principio di località, è probabile che il processore richieda in un secondo momento locazioni vicine (_località spaziale_).
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**4. Cicli e iterazioni:**
I loop rispettano il principio di località temporale perché:
- Le stesse istruzioni vengono eseguite ripetutamente
- Le stesse variabili vengono accedute più volte

**Quando i principi di località sono violati:**

**1. Accessi casuali alla memoria:**
Pattern di accesso completamente randomici violano entrambi i principi.

**2. Conflitti nella cache a indirizzamento diretto:**
> Questo tipo di _cache_ è particolarmente poco efficente quando cerchiamo di accedere a due _cacheline_ in memoria allineate naturalmente alla dimensione della _cache_.
> In questo caso ogni accesso causa una `miss`, proprio perché i due indirizzi collidono.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**3. Strutture dati disperse:**
- **Liste linkate**: ogni nodo può essere in una posizione completamente diversa in memoria
- **Hash table con collision chaining**: accessi sparsi basati su funzioni hash
- **Alberi con allocazione non contigua**: attraversamento che richiede salti in memoria casuali

**4. Accessi a grandi strutture dati:**
Quando si accede agli elementi di un array con grandi intervalli (nel nostro caso più grandi di `64Byte`), si viola la località spaziale.

**5. Operazioni di I/O:**
> La memoria cache lavora _solo_ sulla **RAM**, non ha alcun senso che lavori per l'_I/O_, poiché manca il principio base (_I/O_ ha effetti collaterali voluti).
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**6. Problemi con cache associative:**
Anche con cache più sofisticate, i principi possono essere violati:

> Tuttavia anche con il registro `LRU` è possibile generare sempre `miss`.
> Basta infatti effettuare un accesso in più di quelli possibili in parallelo, ad esempio se avessimo quattro cache e facessimo l'accesso a 5 linee allineate, genereremmo sempre una `miss`.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#32-cache-associative-ad-insiemi)*

**Esempi pratici di violazione:**

**Pattern di accesso che generano sempre miss:**
Un esempio concreto è l'accesso a indirizzi che mappano sempre sulla stessa linea di cache:

> In particolare, le sezioni che possono fare conflitto in una stessa locazione sono ${\dim{(\mathbf{RAM})} \over \dim{(\mathbf{cache})}}$, ovvero quelle **allineate naturalmente** a `l`, con `l` che indica il numero di _cache line_ disponibili.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Accessi non allineati:**
Gli accessi non allineati possono causare miss aggiuntivi perché richiedono accessi multipli:

> Gli accessi non allineati hanno un costo significativo perché:
> - Richiedono **doppi accessi** alla memoria
> - Aumentano il **traffico sul bus**
> - Possono causare **miss aggiuntivi** nella cache
> - Richiedono **elaborazione extra** da parte del μ-codice
>
> *Fonte: [Domande e Risposte.md](./Domande%20e%20Risposte#domanda-14-answered)*

**Implicazioni per i programmatori:**

**Ottimizzazione basata sui principi:**
> Tuttavia il programmatore può _approfittare dei principi_, affinché possa sfruttare al massimo la _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Strutture dati cache-friendly:**
- Preferire array a liste linkate quando possibile
- Organizzare i dati per accesso sequenziale
- Utilizzare blocking e tiling negli algoritmi di matrix multiplication
- Minimizzare gli accessi sparsi ai dati

**Conclusione:**
I principi di località sono **osservazioni statistiche** sui pattern comuni di accesso alla memoria, ma **non sono leggi universali**. La loro validità dipende dall'algoritmo specifico e dalla struttura dei dati utilizzata. I programmatori consapevoli possono progettare algoritmi e strutture dati che li rispettano per massimizzare l'efficacia della cache, mentre alcuni algoritmi per loro natura li violano sistematicamente.

---

### Domanda 2.3 (answered)
**Domanda:** Come funziona la cache a indirizzamento diretto? Descriva l'implementazione.

**Risposta:**

La cache a indirizzamento diretto è il tipo più semplice di cache, dove ogni indirizzo di memoria ha una sola posizione possibile nella cache determinata da una funzione matematica.

**Principio di funzionamento:**

**Meccanismo di base:**
> Il controllore verifica che un dato richiesto dalla CPU sia già stato memorizzato.
> Se lo è stato lo invia immediatamente, altrimenti effettua una lettura in **RAM** e lo invia alla **CPU**.
> Prima di inviarlo però lo salva localmente, eventualmente rimpiazzando altri dati che erano già salvati.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#3-memoria-cache)*

**Scomposizione dell'indirizzo:**

La cache divide ogni indirizzo di memoria in tre parti fondamentali:

> Ipotizzando che la _cache_ abbia `l` locazioni, otteniamo che i bit di riga sono gli utlimi `k = log(l)` (tolti gli ultimi `3` che identificano la riga all'interno del blocco).
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Struttura dell'indirizzo in cache:**
- **Offset (3 bit)**: I bit `[2:0]` identificano il byte specifico all'interno della cacheline (8 byte per cacheline)
- **Index (k bit)**: I bit `[2+k:3]` determinano la riga specifica della cache dove può essere memorizzato il dato
- **Tag (n-k-3 bit)**: I bit rimanenti dell'indirizzo servono per identificare univocamente quale blocco di memoria è memorizzato in quella riga

**Implementazione hardware:**

**Struttura della cacheline:**
> Le etichette sono quindi grandi `n-k-3+2 bit`, poiché l'ultimo bit, detto `validity`, serve per capire se l'etichetta in quella riga è valida o contiene valori casuali, mentre il penultimo, detto `dirty`, serve per ottimizzare i tempi in caso di scritture in **RAM** dovute a rimpiazamenti di _cacheline_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Campi di ogni entrata del controllore di etichette:**
1. **Tag**: Identifica quale blocco di memoria è memorizzato
2. **Validity bit (V)**: Indica se il contenuto della cacheline è valido
3. **Dirty bit (D)**: Indica se la cacheline è stata modificata rispetto alla RAM

Nella _cacheline_ si hanno i dati effettivi (tipicamente 64 byte)

**Algoritmo di funzionamento:**

**Procedura di accesso in lettura:**
1. **Estrazione dell'index**: Usa i bit dell'index per selezionare la riga della cache
2. **Confronto del tag**: Confronta il tag dell'indirizzo con quello memorizzato nella riga
3. **Controllo validity**: Verifica che il bit V sia settato
4. **Hit/Miss decision**:
   - **Hit**: Se tag coincide e V=1, restituisce il dato dalla cache
   - **Miss**: Se tag non coincide o V=0, accede alla RAM

**Gestione degli accessi in scrittura:**

**Politiche in caso di Hit:**
> In caso di scrittura con `hit` abbiamo due possibili politiche:
> - `Write Through`: scrive il nuovo valore sia in _cache_ che in **RAM**;
> - `Write Back`: scrive il nuovo valore solo nella _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Politiche in caso di Miss:**
> Anche in caso di `miss` la scrittura ha due possibili politiche:
> - `Write Allocate`: copia l'elemento dalla **RAM** nella _cache_ prima della modifica, e poi lo ritrasmette aggiornato;
> - `Write No-Allocate`: effettua l'aggiornamento **solo** in **RAM**, senza salvare nulla in _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Gestione del dirty bit:**
> Per migliorare il tempo si aggiunge un'ulteriore `bit` alle etichette chiamato `D` (_Dirty_) che identifica se in una determinata _cache-line_ sono avvenute o meno scritture.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Vantaggi dell'implementazione:**

**1. Semplicità:**
- Hardware minimo richiesto
- Decisione hit/miss rapida (un solo confronto)
- Accesso diretto alla riga senza ricerca

**2. Velocità:**
- Tempo di accesso costante O(1)
- Nessuna ricerca associativa necessaria
- Implementazione efficiente in circuiti integrati

**3. Ottimizzazione per località spaziale:**
> Il motivo per il quale, data una riga, recuperiamo in cache tutta la sezione dov'è contenuta è perché, per il principio di località, è probabile che il processore richieda in un secondo momento locazioni vicine (_località spaziale_).
> Inoltre, se il tempo di lettura di una riga fosse $t$, quella di lettura di un blocco è un tempo $\ll 8t$.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Limitazioni e problemi:**

**Conflitti tra indirizzi allineati:**
> Con questo tipo di _cache_, detta **_ad indirizzamento diretto_**, si possono generare conflitti tra sezioni.
> In particolare, le sezioni che possono fare conflitto in una stessa locazione sono ${\dim{(\mathbf{RAM})} \over \dim{(\mathbf{cache})}}$, ovvero quelle **allineate naturalmente** a `l`, con `l` che indica il numero di _cache line_ disponibili.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Problema dei conflitti sistematici:**
> Questo tipo di _cache_ è particolarmente poco efficente quando cerchiamo di accedere a due _cacheline_ in memoria allineate naturalmente alla dimensione della _cache_.
> In questo caso ogni accesso causa una `miss`, proprio perché i due indirizzi collidono.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Esempio pratico di conflitto:**
Se la cache ha `8 KiB` righe (2^13), gli indirizzi `0x0000` e `0x1000` mapperanno sulla stessa riga perché differiscono solo nei bit superiori (tag), ma hanno lo stesso index. Questo causa **thrashing** quando si accede alternativamente a questi indirizzi.

**Ottimizzazioni per le scritture:**

**Write-back vs Write-through:**
> Per quanto riguarda la `write-back`, la scrittura verrà comunque eseguita in **RAM** prima o poi, nel peggiore dei casi quando quella _cache line_ viene sostituita.
> Il guadagno del non fare direttamente il `write-through` si vede quando effettuiamo un numero molto elevato di scritture nella stessa _cache-line_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Algoritmo di sostituzione:**
Nella cache a indirizzamento diretto non c'è scelta: quando serve spazio per una nuova cacheline, quella esistente viene sempre sostituita. Se il dirty bit è settato, viene prima eseguito il write-back in RAM.

**Confronto con cache associative:**

La **soluzione** ai problemi di conflitto è rappresentata dalle cache associative:
> Un modo per risolvere il problema è attraverso le **cache associative ad insiemi**.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Collegamenti con altre domande:**
- **Domanda 2.6**: Le cache associative per insiemi risolvono i problemi della cache a indirizzamento diretto

**Conclusioni:**
La cache a indirizzamento diretto rappresenta la forma più semplice ed efficiente di cache in termini di hardware, ma soffre di conflitti sistematici che possono degradare le prestazioni in scenari specifici. La sua semplicità la rende ideale per implementazioni dove il costo e la velocità sono prioritari rispetto alla flessibilità.

---

### Domanda 2.4 (answered)
**Domanda:** Come è fatta una cacheline? Quali informazioni contiene e come viene gestita?

**Risposta:**

Una **cacheline** è l'unità fondamentale di trasferimento e memorizzazione nella memoria cache, composta da **dati effettivi** e **metadati di controllo** che permettono la gestione efficiente delle operazioni di lettura e scrittura.

_**Struttura di una Cacheline**_

_**Componenti Principali**_

**1. Dati effettivi:**
La cacheline contiene un **blocco di dati contigui** dalla memoria principale, tipicamente di **64 byte** (8 parole da 64 bit). Questo blocco viene recuperato per intero dalla RAM quando si verifica un miss, sfruttando il principio di località spaziale.

> Il motivo per il quale, data una riga, recuperiamo in cache tutta la sezione dov'è contenuta è perché, per il principio di località, è probabile che il processore richieda in un secondo momento locazioni vicine (_località spaziale_).
> Inoltre, se il tempo di lettura di una riga fosse $t$, quella di lettura di un blocco è un tempo $\ll 8t$.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**2. Metadati di controllo (etichette):**

> Le etichette sono quindi grandi `n-k-3+2 bit`, poiché l'ultimo bit, detto `validity`, serve per capire se l'etichetta in quella riga è valida o contiene valori casuali, mentre il penultimo, detto `dirty`, serve per ottimizzare i tempi in caso di scritture in **RAM** dovute a rimpiazamenti di _cacheline_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**_Campi di Controllo Dettagliati_**

**1. Tag (n-k-3 bit):**
- Identifica **quale blocco specifico** di memoria è memorizzato nella cacheline
- Permette di distinguere tra blocchi diversi che mappano sulla stessa riga della cache
- Utilizzato per il confronto durante la decisione hit/miss

**2. Validity Bit (V):**
- **1 bit** che indica se il contenuto della cacheline è **valido**
- `V=1`: La cacheline contiene dati validi dalla memoria
- `V=0`: La cacheline è vuota o contiene dati non significativi (inizializzazione o invalidazione)

**3. Dirty Bit (D):**
> Per migliorare il tempo si aggiunge un'ulteriore `bit` alle etichette chiamato `D` (_Dirty_) che identifica se in una determinata _cache-line_ sono avvenute o meno scritture.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

- **1 bit** che indica se la cacheline è stata **modificata** rispetto alla RAM
- `D=1`: La cacheline contiene dati aggiornati non ancora scritti in RAM
- `D=0`: La cacheline è sincronizzata con la RAM

_**Gestione delle Cacheline**_

_**- Operazioni di Lettura**_

**Procedura di accesso:**
1. **Estrazione dell'index**: Usa i bit dell'index dell'indirizzo per selezionare la riga
2. **Confronto del tag**: Confronta il tag dell'indirizzo con quello memorizzato
3. **Controllo validity**: Verifica che `V=1`
4. **Decisione hit/miss**:

> Il segnale di `hit` indica che la memoria si trova già nella _cache_, perciò è sufficente leggere quella.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

_**- Operazioni di Scrittura**_

**In caso di HIT:**
> In caso di scrittura con `hit` abbiamo due possibili politiche:
> - `Write Through`: scrive il nuovo valore sia in _cache_ che in **RAM**;
> - `Write Back`: scrive il nuovo valore solo nella _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**In caso di MISS:**
> Anche in caso di `miss` la scrittura ha due possibili politiche:
> - `Write Allocate`: copia l'elemento dalla **RAM** nella _cache_ prima della modifica, e poi lo ritrasmette aggiornato;
> - `Write No-Allocate`: effettua l'aggiornamento **solo** in **RAM**, senza salvare nulla in _cache_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

_**- Gestione del Dirty Bit**_

**Write-back ottimizzata:**
> Per quanto riguarda la `write-back`, la scrittura verrà comunque eseguita in **RAM** prima o poi, nel peggiore dei casi quando quella _cache line_ viene sostituita.
> Il guadagno del non fare direttamente il `write-through` si vede quando effettuiamo un numero molto elevato di scritture nella stessa _cache-line_.
>
> *Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Algoritmo di sostituzione:**
Quando una cacheline dirty deve essere sostituita:
1. **Controllo del dirty bit**: Se `D=1`, esegue write-back in RAM
2. **Invalidazione**: Setta `V=0`
3. **Caricamento**: Carica la nuova cacheline dalla RAM
4. **Aggiornamento metadati**: Setta nuovo tag, `V=1`, `D=0`

_**- Interazione con DMA**_

**Problemi di coerenza:**
> In questa politica le scritture della **CPU** vengono mantenute soltanto in _cache_ e effettuate in maniera _sincrona_ in secondi momenti (come quando la `cacheline dirty` verrebbe sovrascritta).
> Le `cacheline ~dirty` invece continuano a contenere _le stesse informazioni_ della **RAM**.
>
> *Fonte: [DMA.md](./DMA#212-cache-con-politica-write-back)*

**Soluzioni hardware (Intel):**
> Se le linee di controllo identificano un operazione di _scrittura_, il controllore può usare il contenuto delle linee di indirizzo per eseguire una normale ricerca in _cache_, e **nel caso di `hit` invalidare in autonomia la corrispondente `cacheline`**.
>
> *Fonte: [DMA.md](./DMA#211-politica-write-through)*

_**Vantaggi della Struttura**_

**1. Efficienza spaziale:**
- Una cacheline contiene **multipli byte adiacenti**, sfruttando la località spaziale
- Riduce il numero di accessi alla RAM lenta

**2. Gestione trasparente:**
- I bit di controllo permettono gestione automatica senza intervento del programmatore
- Ottimizzazioni hardware per write-back e invalidazioni

**3. Coerenza dei dati:**
- Il dirty bit previene perdite di dati durante le sostituzioni
- Il validity bit garantisce l'integrità dei contenuti

La cacheline rappresenta quindi un'**unità di memorizzazione intelligente** che bilancia efficienza, coerenza e semplicità di gestione hardware, costituendo il mattone fondamentale dell'architettura delle memorie cache moderne.

---

### Domanda 2.5
**Domanda:** Descriva le politiche di sostituzione delle cacheline e i loro vantaggi/svantaggi. Spiega LRU e pseudo-LRU.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 2.6 (answered)
**Domanda:** Come funziona la cache associativa per insiemi? Confronti con quella a indirizzamento diretto.

**Risposta:**
La cache associativa per insiemi è un'evoluzione della cache a indirizzamento diretto che risolve il problema dei conflitti tra cacheline allineate naturalmente.

**Cache a indirizzamento diretto - problemi:**

> Con questo tipo di _cache_, detta **_ad indirizzamento diretto_**, si possono generare conflitti tra sezioni.
> In particolare, le sezioni che possono fare conflitto in una stessa locazione sono ${\dim{(\mathbf{RAM})} \over \dim{(\mathbf{cache})}}$, ovvero quelle **allineate naturalmente** a `l`, con `l` che indica il numero di _cache line_ disponibili.

> Questo tipo di _cache_ è particolarmente poco efficente quando cerchiamo di accedere a due _cacheline_ in memoria allineate naturalmente alla dimensione della _cache_.
> In questo caso ogni accesso causa una `miss`, proprio perché i due indirizzi collidono.

*Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#31-cache-ad-indirizzamento-diretto)*

**Funzionamento della cache associativa per insiemi:**

> Si basano sulle _cache ad indirizzamento diretto_.
> Infatti non sono altro che più cache allineate tra di loro:
>
> L'esempio a destra è con due _cache_.
>
> Si utilizzano le due cache in parallelo, in caso di conflitti, andremo a sovrascrivere la _cacheline_ che non si utilizza da più tempo.

*Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#32-cache-associative-ad-insiemi)*

**Gestione della sostituzione - Registro LRU:**

> Oltre alle cache, si introduce anche un ulteriore registro `R` che contiene tanti bit quanti sono necessari per ricordare l'ordinamento delle scritture.
>
> In caso di due vie in `R` è sufficente `1bit`, che codifica quale delle due _cache_ non si utilizza da più tempo in quella _line_.
> In caso di quattro vie il registro viene chiamato `LRU` e contiene `5bit`.
>
> Tuttavia, nel processore _x86_, è stato implementato uno `pseudo-LRU` a `3bit`

*Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#32-cache-associative-ad-insiemi)*

**Limitazioni della cache associativa per insiemi:**

> Tuttavia anche con il registro `LRU` è possibile generare sempre `miss`.
> Basta infatti effettuare un accesso in più di quelli possibili in parallelo, ad esempio se avessimo quattro cache e facessimo l'accesso a 5 linee allineate, genereremmo sempre una `miss`.

*Fonte: [Memoria e Periferiche.md](./Memoria%20e%20Periferiche#32-cache-associative-ad-insiemi)*

**Vantaggi principali del confronto:**
- **Cache a indirizzamento diretto**: Semplice, veloce, ma soffre di conflitti quando si accede a indirizzi allineati naturalmente
- **Cache associativa per insiemi**: Risolve i conflitti grazie alle multiple "vie" parallele e al sistema LRU, ma aumenta la complessità hardware

---

### Domanda 2.7
**Domanda:** Cosa sono le politiche `write-through` e `write-back`? Quando si usano?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 2.8
**Domanda:** Come interagisce la cache con la memoria virtuale e la paginazione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 3. Interruzioni

### Domanda 3.1 (answered)
**Domanda:** Spieghi il problema di Dijkstra relativo alla stampa e come le interruzioni lo risolvono. Perché un approccio con `checkFlag()` troppo frequente o troppo raro non è ottimale?

**Risposta:**
Il problema di Dijkstra è un classico esempio che dimostra la necessità delle interruzioni per gestire efficacemente la sincronizzazione tra CPU e periferiche.

**Il problema originale:**
> Ipotizziamo di avere una tabella contenente due indici:
> - Nel primo si trova un numero $x$ crescente
> - Nel secondo si trova il valore di una funzione $f(x)$ calcolata sul valore di $x$ attuale.
>
> Vorremmo che venissero stampati le coppie valore $x$-$f(x)$.
>
> *Fonte [Interruzioni.md](./Interruzioni#2-interruzioni)*

**Architettura della stampante:**
> La stampa degli elementi è gestita tramite una stampante con due registri:
> - `TBR`: contiene il valore di $f(x)$
> - `STS`: _handshake_, contiene un flag che indica che la stampa di $f(x)$ è avvenuta con successo e si può inserire un nuovo valore in `TBR`.

**Approccio ingenuo con polling:**
> Un approccio al problema potrebbe essere questo:
> 1. Calcolo $f(x_1)$
> 2. Lo inserisco nella stampante
> 3. Attendo che la stampa avvenga spettando `checkFlag()`
> 4. Calcolo $f(x_2)$
> 5. ...
>
> Questo approccio però perde del tempo durante l'attesa di `checkFlag()`.
> In particolare potremmo utilizzare questo tempo per precalcolare le $f(x_i)$ successive.

**Problemi del polling con frequenza inadeguata:**

*Troppo raro:*
```cpp
checkFlag();
for(int i = 0; i < 1000000; ++i)
	a += i;
checkFlag();
```
- **Problema**: La stampante potrebbe completare la stampa molto prima del prossimo controllo
- **Conseguenza**: Spreco di tempo prezioso che potrebbe essere usato per calcoli utili

*Troppo frequente:*
```cpp
for(int i = 0; i < 1000000; ++i){
	checkFlag()
	a += i;
}
```
- **Problema**: Troppo tempo speso a controllare il flag invece di fare calcoli produttivi
- **Conseguenza**: Overhead eccessivo che rallenta l'elaborazione

**Soluzione con interruzioni:**
> Implementiamo quindi una modifica **<u>hardware</u>** che ci permetta poi di implementare le _routine_ degli eventi in _software_.

**Meccanismo hardware:**
> Quello che possiamo fare per supportare gli eventi di questa interfaccia è collegare fisicamente il bit della stampante alla **CPU** e aggiungere una $\mu$-istruzione che controlla il bit al termine di ogni istruzione.

**Vantaggi delle interruzioni:**
1. **Efficienza**: La CPU può continuare a calcolare le funzioni $f(x)$ senza perdere tempo in polling
2. **Reattività**: Risposta immediata quando la stampante è pronta
3. **Ottimizzazione**: Utilizzo ottimale delle risorse di sistema

**Gestione della singola richiesta:**
> Il processore vede continuamente il segnale `READY` come settato ricevendo di fatto infinite richieste, quando in realtà vogliamo solamente una richiesta singola.
>
> Per rimediare a questi problemi è sufficente inserire un **generatore di impulsi** e un **`FF-SR`** che verrà resettato quando la richiesta sarà stata già presa in **attenzione**.

**Controllo del flusso delle interruzioni:**
> Per quanto riguarda la gestione delle interruzioni durante _routine_, nel processore _intelx86_ esiste un **flag aggiuntivo** in `RFLAG`, chiamato `IF` (_Interrupt Flag_). Se il bit è resettato, il processore **non accetta nuove richieste** finché non ha terminato quella attuale.

Questo approccio risolve elegantemente il problema di Dijkstra permettendo alla CPU di massimizzare l'utilizzo computazionale mentre mantiene una sincronizzazione perfetta con le periferiche.

---

### Domanda 3.2 (answered)
**Domanda:** Descriva il meccanismo hardware delle interruzioni `APIC`. Come viene risolto il problema delle interruzioni a più sorgenti?

**Risposta:**
L'APIC (Advanced Programmable Interrupt Controller) è un controllore delle interruzioni che risolve il problema della gestione di più sorgenti di interrupt in modo elegante ed efficiente.

**Architettura hardware dell'APIC:**

> Il _controllore_ è collegato alle periferiche tramite tre fili, ognuna collegata ad un **_piedino noto dalle specifiche_**.
> Nel nostro calcolatore i dispositivi rilevanti sono connessi ai seguenti piedini
> - **Tastiera** ↔ `1`
> - **Timer** ↔ `2`
> - **Hard Disk** ↔ `14`
>
> Quando uno di questi segnali viene settato l'`APIC` invia alla **CPU** un segnale tramite un suo registro interno chiamato `INTR` (_INTervall Request_), inizializzando un _handshake_.

*Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Meccanismo di gestione delle interruzioni multiple:**

L'APIC risolve il problema delle interruzioni a più sorgenti attraverso diversi registri specializzati da 256 bit:

> Per gestire le richieste di interruzione, l'`APIC`, oltre a `EOI`, possiede altri due registri a `256bit`:
> - `ISR` (_In Service Register_): conserva informazioni relative ai tipi accettati dalla **CPU** e non ancora terminati.
> - `IRR` (_Interrupt Request Register_): conserva informazioni relative alle richieste dei tipi non ancora accettati.

*Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Sistema di priorità e tipizzazione:**

Il programmatore assegna un **tipo** (8 bit) a ogni piedino dell'APIC:

> Il programmatore ha quindi il compito di assegnare una **precedenza** alle varie richieste, e lo fa tramite il **tipo**.
>
> Quando assegna un tipo il programmatore ha `8bit`, dove i 4 più significativi indicano la _classe di precedenza_.
>
> Se arriva una nuova richiesta che ha _classe strettamente maggiore_ l'`APIC` invierà una **nuova richiesta**, negli altri casi attenderà `EOI`, per poi inviare la successiva richiesta con classe più alta in `IRR`.

*Fonte: [Interruzioni.md](./Interruzioni#211-gestione-più-richieste)*

**Protocollo di handshake:**

> Per permettere di capire chi è la sorgente del segnale, **il programmatore** associa ad ogni piedino del controllore `APIC` un _tipo_, ovvero una codifica su `8bit`.
>
> Nel momento in cui la **CPU** accetta la richiesta, si prepara inviando il segnale di _handshake_ tramite il filo `INTA`: _INTervall Acknowledge_.
>
> Successivamente l'`APIC` carica il tipo sul _bus_ così che la **CPU** lo possa leggere.

*Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Configurazione software dell'APIC:**

Dal file [`IO.md`](./IO#3-modulo-io):

```cpp
// Associazione irq->tipo (tramite l'APIC)
apic::set_VECT(irq, tipo);
apic::set_MIRQ(irq, false);  // abilita l'interruzione
gate_init(tipo, routine);    // associa routine al tipo
```

L'APIC risolve così il problema delle interruzioni multiple attraverso: vectorizzazione delle interruzioni per identificare la sorgente, sistema di priorità basato sui tipi, registri ISR/IRR per tracking delle interruzioni, e protocollo EOI per la sincronizzazione.

---

### Domanda 3.3 (answered)
**Domanda:** Spieghi il ruolo dei registri `ISR` (In Service Register) e `IRR` (Interrupt Request Register) nell'`APIC`. Come funziona il meccanismo di `EOI` (End Of Interrupt)?

**Risposta:**
I registri ISR e IRR sono componenti fondamentali dell'APIC per la gestione delle interruzioni multiple e la loro sincronizzazione.

**Funzione dei registri APIC:**
> Per gestire le richieste di interruzione, l'`APIC`, oltre a `EOI`, possiede altri due registri a `256bit`:
> - `ISR` (_In Service Register_): conserva informazioni relative ai tipi accettati dalla **CPU** e non ancora terminati.
> - `IRR` (_Interrupt Request Register_): conserva informazioni relative alle richieste dei tipi non ancora accettati.
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Dispositivi collegati:**
> Nel nostro calcolatore i dispositivi rilevanti sono connessi ai seguenti piedini
> - **Tastiera** ↔ `1`
> - **Timer** ↔ `2`
> - **Hard Disk** ↔ `14`

**Meccanismo di handshake:**
> Quando uno di questi segnali viene settato l'`APIC` invia alla **CPU** un segnale tramite un suo registro interno chiamato `INTR` (_INTervall Request_), inizializzando un _handshake_.

**Meccanismo EOI (End Of Interrupt):**
> Per evitare comportamenti indesiderati da parte dell'`APIC` è importante che questo `bit` venga settato **quando tutta la routine è terminata** e non c'è altro da fare.
> Se così non fosse, il controllore potrebbe reinviare segnali di interruzioni su eventi già gestiti.

**Esempio pratico nel codice:**
Dal file [`IO.md`](./IO#3-modulo-io) - Configurazione APIC:
```cpp
// Associazione irq->tipo (tramite l'APIC)
apic::set_VECT(irq, tipo);
// Smascheriamo le richieste irq nell'APIC
apic::set_MIRQ(irq, false);
```

E nell'handler:
```x86asm
a_wfi:
	CALL salva_stato
	CALL apic_send_EOI  ; ← Invio EOI all'APIC
```

**Funzionamento dettagliato:**
1. **IRR**: Tiene traccia delle richieste ricevute ma non ancora elaborate
2. **ISR**: Tiene traccia delle interruzioni attualmente in elaborazione
3. **EOI**: Segnala la fine dell'elaborazione e permette all'APIC di elaborare nuove richieste

---

### Domanda 3.4 (answered)
**Domanda:** Descriva il funzionamento delle interruzioni e la scrittura delle routine. Come mai si usa `int` e non `call` per le interruzioni?

**Risposta:**
Il funzionamento delle interruzioni e la scrittura delle routine coinvolgono meccanismi hardware e software complessi per garantire una gestione sicura ed efficiente degli eventi asincroni.

**Funzionamento hardware delle interruzioni:**

**Controllo periodico:**
> Quello che possiamo fare per supportare gli eventi di questa interfaccia è collegare fisicamente il bit della stampante alla **CPU** e aggiungere una $\mu$-istruzione che controlla il bit al termine di ogni istruzione.
>
> *Fonte: [Interruzioni.md](./Interruzioni#2-interruzioni)*

**Salvataggio dello stato:**
> Quando il processore _accetta un'interruzione_ salva nella pila diverse informazioni, tra le quali:
> - `RIP` per capire dove tornare al termine della routine
> - `RFLAG` per poter poi ripristinare i flag come se la routine non fosse mai avvenuta.
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Gestione delle interruzioni multiple - APIC:**

**Meccanismo di identificazione:**
> Per permettere di capire chi è la sorgente del segnale, **il programmatore** associa ad ogni piedino del controllore `APIC` un _tipo_, ovvero una codifica su `8bit`.
>
> Nel momento in cui la **CPU** accetta la richiesta, si prepara inviando il segnale di _handshake_ tramite il filo `INTA`: _INTervall Acknowledge_.
>
> Successivamente l'`APIC` carica il tipo sul _bus_ così che la **CPU** lo possa leggere.
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Collegamento dispositivi:**
> Nel nostro calcolatore i dispositivi rilevanti sono connessi ai seguenti piedini
> - **Tastiera** ↔ `1`
> - **Timer** ↔ `2`
> - **Hard Disk** ↔ `14`
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Scrittura delle routine di interruzione:**

**Struttura base delle routine:**
```x86asm
#include <libce.h>
	.global a_tastiera
	.extern c_tastiera
a_tastiera:
	salva_registri
	call c_tastiera
	carica_registri
	iretq
```
*Fonte: [Interruzioni.md](./Interruzioni#212-gestione-corretta-delle-routine)*

**Regole fondamentali:**
> Nelle routine ci sono alcune regole da seguire:
> 1. Ricordarsi di settare e resettare correttamente `IF` tramite le istruzioni `STI` e `CLI`
> 1. Utilizzare l'istruzione `IRETq` piuttosto che `RET`. `IRETq` infatti si ripristina lo stato del processore a prima che la routine iniziasse, in particolare ripristina `RFLAG` e gli altri registri prima di ripristinare `RIP`.
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Salvataggio dei registri:**
> Per ovviare anche a questo la soluzione è quella di salvare il contenuto di **tutti i registri** nella pila.
> In `assembly` su `64bit` non esistono equivalenti della `PUSHAD`/`POPAD`, ma possiamo utilizzare una macro messa a disposizione dalla libreria
>
> *Fonte: [Interruzioni.md](./Interruzioni#212-gestione-corretta-delle-routine)*

**Perché si usa `INT` e non `CALL`:**

**1. Cambio di contesto e protezione:**
La differenza fondamentale tra `INT` e `CALL` riguarda la **protezione del sistema** e il **cambio di contesto** tra livelli di privilegio.

**Motivazione della protezione:**
> Per far funzionare ciò dobbiamo quindi _togliere agli utenti la possibilità di poter sfruttare le interruzioni_, permettendo loro però di poter comunque chiamare le _routine_ e utilizzarle.
>
> La _intel_ ha adottato un sistema diverso, introducendo un nuovo operando assembler `INT $tipo` che fa da _gate_ per chiamare la _routine_ (_primitiva di sistema_) e passare in modalità `sistema`.
> `$tipo` è un numero tra `0` e `255`, ed ha lo stesso significato del tipo delle eccezioni e delle interruzioni esterne.
>
> *Fonte: [Protezione.md](./Protezione#2-protezione)*

**2. Meccanismo di attraversamento del gate:**
Quando si usa `INT`, il processore esegue una sequenza complessa:

> 1. Innanzitutto il processore si procura il _tipo_ dell'interruzione
> 2. Verifica se il bit `P` associato al tipo è zero, generando un'eccezione di _gate non presente_ `11` in caso positivo
> 3. Se sta gestendo una _interruzione software_ o `int3`, confronta il livello corrente con il campo `DPL` del gate.
> 4. Altrimenti, confronta `CS` con `L`. Se `L` è **inferiore**, si genera ancora un'_eccezione di protezione_ `13`.
> 5. Negli altri casi, il processore salva in un registro di appoggio (chiamiamolo `SRSP`) il contenuto corrente di `RSP`
> 6. Se `CS` è diverso da `L` esegue un _cambio di pila_
> 7. Salva in pila `5 long word` [SS, SRSP, RFLAGS, CS, RIP]
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**3. Controllo di accesso:**
La IDT fornisce un controllo granulare degli accessi:

> Ogni _gate_ della `IDT` occupa `16Byte` e contiene le segueni informazioni:
> - Il puntatore alla `Routine` a cui saltare (`8 Byte`)
> - `P` (_Presenza_): indica se la riga contiene bit significativi
> - `I/T`: indica se il _gate_ è di tipo _Interrupt_ (azzera `IF`) o _Trap_ (mantiene `IF` invariato).
> - `L` (_Livello_): indica il _livello di privilegio_ al quale portare il processore **dopo** aver passato il _gate_. Nel nostro caso sarà sempre settato a `sistema`.
> - `DPL` (_Descriptor Privilege Level_): specifica il **livello di privilegio minimo** che deve avere il processore **prima** di passare il gate.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**4. Differenze principali tra INT e CALL:**

| Aspetto                   | `CALL`             | `INT`                            |
| ------------------------- | ------------------ | -------------------------------- |
| **Livello di privilegio** | Rimane invariato   | Può aumentare (utente → sistema) |
| **Pila**                  | Usa la stessa pila | Cambia pila se necessario        |
| **Protezione**            | Nessun controllo   | Controlli DPL e protezione       |
| **Salvataggio stato**     | Solo RIP           | RIP, CS, RFLAGS, RSP, SS         |
| **Ritorno**               | `RET`              | `IRETQ`                          |
| **Accesso IDT**           | No                 | Sì, tramite `$tipo`              |

**5. Necessità del cambio pila:**
> Il cambio di pila è **_necessario_**, è ha due motivazioni:
> - Il processore deve garantire di poter scrivere le 5 `long word` senza sovrascrivere altre cose, e non può quindi fidarsi di `RSP` che è completamente a servizio dell'utente.
> - Queste informazioni sono salvate nella memoria di sistema, in modo che l'utente **non le possa corrompere**.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Configurazione software delle interruzioni:**

**Esempio di configurazione:**
```cpp
// Associazione irq->tipo (tramite l'APIC)
apic::set_VECT(irq, tipo);
// Associazione tipo->handler (tramite la IDT)
gate_init(tipo, routine);
// Abilitazione interruzioni
apic::set_MIRQ(irq, false);
```
*Fonte: [IO.md](./IO#3-modulo-io)*

**Gestione dell'End Of Interrupt:**
> Per evitare comportamenti indesiderati da parte dell'`APIC` è importante che questo `bit` venga settato **quando tutta la routine è terminata** e non c'è altro da fare.
> Se così non fosse, il controllore potrebbe reinviare segnali di interruzioni su eventi già gestiti.
>
> *Fonte: [Interruzioni.md](./Interruzioni#21-interruzioni-a-più-sorgenti---apic)*

**Conclusioni:**
L'istruzione `INT` è fondamentale perché fornisce un meccanismo sicuro e controllato per:
1. **Passare da contesto utente a sistema** mantenendo la protezione
2. **Salvare completamente lo stato** del processore
3. **Cambiare pila** per evitare corruzione dei dati
4. **Controllare l'accesso** tramite i gate della IDT
5. **Gestire le interruzioni asincrone** in modo trasparente

Una semplice `CALL` non può garantire questi requisiti di sicurezza e controllo necessari in un sistema operativo moderno.

---

### Domanda 3.5
**Domanda:** Cosa sono le interruzioni esterne? A che serve la `apic_send_EOI()`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.6
**Domanda:** Descriva l'annidamento delle interruzioni con l'utilizzo dei due registri `ISR` e `IRR`. Cosa comporta programmare con le interruzioni attive?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.7
**Domanda:** Quando si accodano le richieste di interruzione su `IRR`? Perché quando si attraversa il gate viene salvato il registro dei flag?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.8
**Domanda:** Come gestiamo le periferiche che agiscono sullo stesso piedino dell'`APIC`? Quanti piedini ha l'`APIC`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.9
**Domanda:** Chi configura i registri dell'`APIC` e la `IDT`? Chi stabilisce le classi di priorità nelle interruzioni e come si riconoscono?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.10
**Domanda:** Qual è la differenza tra interrupt su fronte e su livello? È possibile che si perdano interruzioni?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.11
**Domanda:** Quando si traduce una funzione in C++ sono ripristinati tutti i registri? Quali sono le istruzioni che ci permettono di usare le interruzioni?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.12
**Domanda:** Come si gestisce il problema del _Cavallo di Troia_ nel contesto delle interruzioni e del sistema di protezione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 3.13
**Domanda:** Cosa fa, dove e come si salta dopo `carica_stato`? Descriva il meccanismo di cambio di contesto.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 4. Eccezioni

### Domanda 4.1 (answered)
**Domanda:** Qual è la differenza fondamentale tra interruzioni ed eccezioni in termini di quando possono essere sollevate durante l'esecuzione di un'istruzione?

**Risposta:**
La differenza fondamentale tra interruzioni ed eccezioni riguarda il **timing** del loro sollevamento rispetto al ciclo di esecuzione delle istruzioni.

**Interruzioni:**
> Quello che possiamo fare per supportare gli eventi di questa interfaccia è collegare fisicamente il bit della stampante alla **CPU** e aggiungere una $\mu$-istruzione che controlla il bit al termine di ogni istruzione.
>
> *Fonte: [Interruzioni.md](./Interruzioni#2-interruzioni)*

Le interruzioni vengono **controllate e accettate solo tra un'istruzione e la successiva**. Il processore verifica la presenza di richieste di interruzione alla fine di ogni istruzione completata, quando il processore si trova in uno stato consistente.

**Caratteristiche delle interruzioni:**
- **Timing**: Solo tra istruzioni complete
- **Asincronia**: Generate da eventi esterni (tastiera, timer, dispositivi I/O)
- **Prevedibilità**: Il processore è sempre in uno stato consistente quando vengono gestite
- **Controllo**: Possono essere mascherate tramite il flag `IF`

**Eccezioni:**
> Mentre le interruzioni possono accedere solo tra un'istruzione e la successiva, le eccezioni possono essere sollevate **in un momento qualunque di un'istruzione** (lettura, decodifica, esecuzione).
>
> *Fonte: [Eccezioni.md](./Eccezioni#22-gestione-eccezioni)*

Le eccezioni possono essere sollevate **durante qualsiasi fase dell'esecuzione di un'istruzione**:
- Durante la **fetch** dell'istruzione
- Durante la **decodifica**
- Durante l'**esecuzione** vera e propria

**Classificazione delle eccezioni per timing:**
Dal file [`Eccezioni.md`](./Eccezioni#22-gestione-eccezioni):

| Tipo    | Quando viene generata                             | Indirizzo salvato                             | Gestione                                                     |
| ------- | ------------------------------------------------- | --------------------------------------------- | ------------------------------------------------------------ |
| `Fault` | Durante l'esecuzione di un'istruzione             | Indirizzo dell'istruzione che stava eseguendo | Permette la ri-esecuzione dell'istruzione dopo la correzione |
| `Trap`  | Tra l'esecuzione di un'istruzione e la successiva | Indirizzo dell'istruzione successiva          | Comportamento simile alle interruzioni                       |
| `Abort` | In qualsiasi momento                              | -                                             | Errori gravi, spesso irreversibili                           |

**Implicazioni pratiche:**
1. **State saving**: Le eccezioni richiedono meccanismi più complessi per salvare lo stato del processore, dato che possono interrompere un'istruzione a metà
2. **Recovery**: I `Fault` permettono di correggere il problema e riprovare l'istruzione che ha causato l'eccezione
3. **Sincronismo**: Le eccezioni sono sempre sincrone rispetto al flusso di istruzioni, mentre le interruzioni sono asincrone

**Esempio pratico:**
- **Interruzione**: Timer che scatta ogni 50ms, controllato solo alla fine di ogni istruzione
- **Eccezione Fault**: Page fault durante l'accesso a memoria di un'istruzione `LOAD` - l'eccezione viene sollevata durante l'esecuzione dell'istruzione
- **Eccezione Trap**: Breakpoint `int3` - viene sollevata come se fosse tra due istruzioni

Questa distinzione è fondamentale per capire come il processore gestisce gli eventi e mantiene la coerenza del sistema.

---

### Domanda 4.2 (answered)
**Domanda:** Come funziona l'eccezione di debug `int3` e come viene utilizzata dai debugger per implementare i breakpoint?

**Risposta:**

L'eccezione di debug `int3` è un meccanismo hardware fondamentale che permette ai debugger di controllare l'esecuzione dei programmi attraverso i breakpoint. È implementata tramite l'eccezione tipo 3 della IDT.

**Caratteristiche dell'eccezione `int3`:**

**Classificazione:**
> - `3`: **Eccezione di debug** (istruzione `int3`)
>
> *Fonte: [Eccezioni.md](./Eccezioni#2-eccezioni)*

L'`int3` è classificato come un'eccezione di tipo **Trap**:

> | Tipo    | Quando viene generata                             | Indirizzo salvato                             | Gestione                                                     |
> | ------- | ------------------------------------------------- | --------------------------------------------- | ------------------------------------------------------------ |
> | `Trap`  | Tra l'esecuzione di un'istruzione e la successiva | Indirizzo dell'istruzione successiva          | Comportamento simile alle interruzioni                       |
>
> *Fonte: [Eccezioni.md](./Eccezioni#22-gestione-eccezioni)*

**Implementazione dei breakpoint:**

**1. Meccanismo base:**
> È grazie a questa che il _debugger_ riesce a controllare il flusso del programma sul quale è eseguito.
> Quando il _debugger_ inserisce un `breakpoint` in un indirizzo, quello che fa operativamente è sostituire il primo byte a quell'indirizzo con il valore `0xcc`, salvando il byte significativo.
>
> *Fonte: [Eccezioni.md](./Eccezioni#21-eccezione-di-debug-int3)*

Il valore `0xcc` corrisponde al codice macchina dell'istruzione `int3` su architettura x86.

**2. Ciclo di esecuzione del breakpoint:**

**Fase 1 - Attivazione del breakpoint:**
> Tramite il segnale di `continue` il debugger rilascia il controllo al programma, che eseguirà finché non farà la _fetch_ dell'eccezione.
> Il controllo torna quindi al _debugger_, che opererà finché il programmatore non restituirà il controllo al programma.
>
> *Fonte: [Eccezioni.md](./Eccezioni#21-eccezione-di-debug-int3)*

**Fase 2 - Gestione della transizione:**
> Prima di permettere al programmatore di agire sul codice, il debugger **reinserisce il vecchio valore** dove aveva salvato `0xcc`, e setta il bit `TF` così da generare un'eccezione di _single-step_.
>
> *Fonte: [Eccezioni.md](./Eccezioni#21-eccezione-di-debug-int3)*

**Fase 3 - Esecuzione dell'istruzione originale:**
> Il programma eseguirà quindi l'operazione dove era stato chiamato il breakpoint per poi dare controllo nuovamente al _debugger_, che reinserirà il valore `0xcc` così da mantenere il `breakpoint` per le successive iterazioni.
> Il _debugger_ resetta quindi `TF` e restituisce per l'ultima volta il controllo al flusso principale
>
> *Fonte: [Eccezioni.md](./Eccezioni#21-eccezione-di-debug-int3)*

**Interazione con il flag TF (Trap Flag):**

Il flag `TF` nell'`RFLAGS` abilita il single-step debugging:

> - `1`: **Single-Step**: viene avviato se il flag `TF` è settato.
>   Il processore genererà quindi un'eccezione alla fine di ogni istruzione eseguita.
>
> *Fonte: [Eccezioni.md](./Eccezioni#2-eccezioni)*

**Gestione hardware dell'eccezione:**

Quando si verifica un'eccezione `int3`, il processore segue la stessa procedura delle altre eccezioni:

> 1. Innanzitutto il processore si procura il _tipo_ dell'interruzione
>    - In caso di _eccezione_ il tipo è implicito;
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Esempio di utilizzo nel nostro sistema:**

Dal contesto del debugger utilizzato nel corso:

> Possiamo notare inoltre che il _debugger_ è **preimpostato per caricare i simboli di tutti e tre i moduli**.
> È quindi possibile **inserire _breakpoint_ liberamente** nel codice del modulo `sistema`, `utente` e `io`.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#243-uso-debugger)*

**Vantaggi del meccanismo `int3`:**

1. **Precisione**: Permette di fermare l'esecuzione esattamente in un punto specifico
2. **Trasparenza**: L'istruzione originale viene preservata e rieseguita correttamente
3. **Efficienza**: Utilizza un'istruzione di un solo byte (0xcc)
4. **Atomicità**: Il breakpoint è gestito completamente a livello hardware
5. **Persistenza**: Il breakpoint rimane attivo per esecuzioni successive

**Differenze con altre forme di debugging:**

- **Polling software**: `int3` non richiede controlli continui nel codice
- **Breakpoint hardware**: `int3` permette un numero illimitato di breakpoint software
- **Emulazione**: Non richiede di emulare istruzioni, solo di sostituire temporaneamente il codice

**Limitazioni:**

- **Codice auto-modificante**: Può interferire con codice che si modifica durante l'esecuzione
- **Memoria di sola lettura**: Richiede che la memoria del programma sia scrivibile per il debugger
- **Concorrenza**: In sistemi multi-thread, richiede sincronizzazione appropriata

Il meccanismo `int3` rappresenta quindi la base hardware fondamentale per tutti i moderni debugger, permettendo di implementare breakpoint efficienti e affidabili senza modificare sostanzialmente l'architettura del programma in esecuzione.

---

### Domanda 4.3
**Domanda:** Che fa la CPU quando rileva un'eccezione? Come fa la CPU a capire dove saltare?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 4.4
**Domanda:** Cosa succede se ho un `page fault` su una istruzione `LOAD`? Come viene gestita questa situazione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 5. Protezione

### Domanda 5.1 (answered)
**Domanda:** Come funzionano i livelli di privilegio nei processori Intel? Quali operazioni sono vietate a livello utente?

**Risposta:**
I livelli di privilegio nei processori Intel x86 sono un meccanismo hardware fondamentale per implementare la **protezione** e garantire la sicurezza del sistema.

> Nel nostro sistema utilizziamo solo due livelli di privilegio:
> - **LIV_SISTEMA**: livello sistema, il più privilegiato
> - **LIV_UTENTE**: livello utente, il meno privilegiato
>
> *Fonte: [Protezione.md](./Protezione)*

Il livello di privilegio corrente è memorizzato nel registro **CS** (Code Selector) e determina quale contesto di esecuzione è attivo.

Il processore distingue tra due contesti principali:

1. **Contesto Sistema**:
   - Ha accesso completo all'hardware
   - Può eseguire tutte le istruzioni privilegiate
   - Può accedere a tutte le aree di memoria

2. **Contesto Utente**:
   - Accesso limitato alle risorse hardware
   - Molte istruzioni sono vietate
   - Accesso limitato alla memoria

> L'idea generale di questo sistema è la seguente:
> 1. All'accensione, tramite il bootstrap, si inizializza il processore a livello `sistema`
> 2. Quando viene inizializzato un job si passa a livello `utente`
> 3. Quando viene generata un'interruzione esterna, si torna al livello `sistema`
> 4. Gestita l'interruzione esterna, si torna al job nel livello `utente`
>
> *Fonte: [Protezione.md](./Protezione)*

Le principali **operazioni vietate** quando il processore opera in contesto utente sono:

- **`IN`** e **`OUT`**: per l'accesso diretto alle porte di I/O
- Queste istruzioni sono controllate dal bit **IOPL** nel registro RFLAGS

> Andremo quindi a **vietare le istruzioni di `IN`, `OUT`, `CLI`, `STI` per il contesto `utente`**, permettendole solamente quando ci si trova nel contesto `sistema`.
>
> *Fonte: [Protezione.md](./Protezione)*

**Controllo delle Interruzioni**
- **`CLI`** (Clear Interrupt Flag): disabilita le interruzioni
- **`STI`** (Set Interrupt Flag): abilita le interruzioni

> Nei processori Intel vi è un'associazione tra `IN` e `OUT` ai comandi `CLI` e `STI`. Se ponessimo il `LIV_UTENTE`, forniremmo l'accesso all'`utente` anche a queste istruzioni, cosa che abbiamo già visto non va fatta.
>
> *Fonte: [IO.md](./IO)*

- **`LIDTR`** (Load Interrupt Descriptor Table Register): carica l'indirizzo della IDT
- **`LGDT`** (Load Global Descriptor Table): carica la GDT
- **`MOV`** verso registri di controllo (CR0, CR2, CR3, CR4)

> Per non permettere la modifica di `IDT` da parte dell'utente l'istruzione `LIDTR` è anch'essa **vietata** nel contesto utente.
>
> *Fonte: [Protezione.md](./Protezione)*

I processi utente non possono accedere direttamente alle aree di memoria riservate al sistema:

> Entrambi i registri sono scrivibili **solo da livello `sistema`**.
>
> *Fonte: [Paginazione.md](./Paginazione)*



Il cambio di livello di privilegio può avvenire solo in modi controllati:

> Il livello di privilegio può essere cambiato solo in due modi:
>
> | Operazione         | Livello di privilegio    |
> | ------------------ | ------------------------ |
> | gate della `IDT`   | `utente` → `sistema`     |
> | Istruzione `IRETQ` | `sistema` → `utente`     |
>
> *Fonte: [Protezione.md](./Protezione)*

Il passaggio da utente a sistema avviene attraverso i **gate della IDT** tramite:
- **Eccezioni** (es. page fault, protezione)
- **Interruzioni** hardware
- **Chiamate di sistema** (`INT` instruction)

IL passaggio da sistema a utente avviene esclusivamente tramite l'istruzione **`IRETQ`** alla fine delle routine di sistema.

Quando il processore incontra un'istruzione privilegiata in contesto utente, genera un'**eccezione di protezione** (tipo 13):

> Il sistema sul quale lavoriamo è progettato affinché qualsiasi eccezione venga sollevata in modalità utente, restituisce il controllo al modulo sistema, il quale **termina forzatamente il processo** e invia alcuni messaggi sul log.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

Per permettere ai processi utente di accedere ai servizi del sistema operativo in modo controllato, vengono utilizzate le **primitive di sistema**:

> Permetteremo all'utente di utilizzare una determinata eccezione (non modificabile nella memoria), salvando in un registro quale routine si vuole chiamare.
> La Intel ha adottato un sistema diverso, introducendo un nuovo operando assembler `INT $tipo` che fa da gate per chiamare la routine (primitiva di sistema) e passare in modalità `sistema`.
>
> *Fonte: [Protezione.md](./Protezione)*

Questo meccanismo garantisce che:
1. I processi utente non possano interferire con il sistema
2. L'accesso alle risorse hardware sia mediato dal kernel
3. La stabilità e sicurezza del sistema siano preservate

I livelli di privilegio rappresentano quindi una **barriera hardware fondamentale** per l'implementazione della sicurezza nei sistemi operativi moderni.

---

### Domanda 5.2
**Domanda:** Cosa sono `M1` e `M2`? Come viene controllato l'accesso alla memoria per livello di privilegio?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 5.3 (answered)
**Domanda:** Come è fatto un gate della IDT? Da dove viene preso il valore del nuovo `%rsp` se si cambia pila mentre si attraversa un gate?

**Risposta:**
I gate della IDT sono strutture hardware fondamentali per la gestione delle interruzioni e il cambio di contesto nei processori Intel, contenenti informazioni cruciali per la protezione e la transizione tra livelli di privilegio.

**Struttura di un gate della IDT:**
> Ogni _gate_ della `IDT` occupa `16Byte` e contiene le segueni informazioni:
> - Il puntatore alla `Routine` a cui saltare (`8 Byte`)
> - `P` (_Presenza_): indica se la riga contiene bit significativi
> - `I/T`: indica se il _gate_ è di tipo _Interrupt_ (azzera `IF`) o _Trap_ (mantiene `IF` invariato).
> - `L` (_Livello_): indica il _livello di privilegio_ al quale portare il processore **dopo** aver passato il _gate_. Nel nostro caso sarà sempre settato a `sistema`.
> - `DPL` (_Descriptor Privilege Level_): specifica il **livello di privilegio minimo** che deve avere il processore **prima** di passare il gate.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Funzione dei campi principali:**

**Campo DPL (Descriptor Privilege Level):**
> Può vietare l'utilizzo di alcuni gate attraverso l'istruzione `INT` generando un'_eccezione di protezione_ `13`.
> I programmatori di sistema possono settarlo come:
> - `sistema`: nei _gate_ delle _interruzioni esterne_, così che possano essere attraversati solo da codice protetto
> - `utente`: nei _gate_ delle _primitive_, per permetterne l'utilizzo da parte degli utenti
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Meccanismo di attraversamento del gate:**
Quando il processore attraversa un gate della IDT, segue una sequenza precisa:

> 1. Innanzitutto il processore si procura il _tipo_ dell'interruzione
> 2. Verifica se il bit `P` associato al tipo è zero, generando un'eccezione di _gate non presente_ `11` in caso positivo
> 3. Se sta gestendo una _interruzione software_ o `int3`, confronta il livello corrente con il campo `DPL` del gate.
> 4. Altrimenti, confronta `CS` con `L`. Se `L` è **inferiore**, si genera ancora un'_eccezione di protezione_ `13`.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Cambio di pila e origine del nuovo `%rsp`:**

**Necessità del cambio pila:**
> Il cambio di pila è **_necessario_**, è ha due motivazioni:
> - Il processore deve garantire di poter scrivere le 5 `long word` senza sovrascrivere altre cose, e non può quindi fidarsi di `RSP` che è completamente a servizio dell'utente.
> - Queste informazioni sono salvate nella memoria di sistema, in modo che l'utente **non le possa corrompere**.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Meccanismo del cambio pila:**
> 5. Negli altri casi, il processore salva in un registro di appoggio (chiamiamolo `SRSP`) il contenuto corrente di `RSP`
> 6. Se `CS` è diverso da `L` esegue un _cambio di pila_ (_pila sistema/utente_ nel nostro caso), caricando un nuovo valore in `RSP`

**Origine del nuovo `%rsp` - Il TSS:**
> Nei primi processori _intel_ ogni _job_ aveva un proprio **segmento** di un registro chiamato `TSS`, che indicava la pila a disposizione del _job_.
> Per identificare la _pila sistema_ si accedeva prima ad un'altro registro, `TR` (_Task Register_), che indicava quale segmento era associato a quel _job_.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Cosa viene salvato in pila:**
> 7. Salva in pila `5 long word`. In ordine:
> - [0] `SS`: `1 long word` non significativa (rimasuglio della segmentazione, ...)
> - [1] `SRSP`: pila salvata al passo 5. Nel caso di cambio pila è quella `utente`, altrimenti punta alla pila `sistema` stessa
> - [2] `RFLAGS`: registro dei flag
> - [3] `CS`: vecchio valore del `CS` da ripristinare successivamente
> - [4] `RIP`: indirizzo della prima istruzione da eseguire all'uscita del gate.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Esempio pratico nella creazione di processi:**
Nel sistema studiato, la pila sistema viene preparata durante la creazione del processo:

> ```cpp
> pl[-6] = int_cast<natq>(f);		      // RIP (codice sistema)
> pl[-5] = SEL_CODICE_SISTEMA;          // CS (codice sistema)
> pl[-4] = BIT_IF;  	        	      // RFLAGS (abilitiamo le interruzioni)
> pl[-3] = fin_sis_p - sizeof(natq);    // RSP (primo elemento della pila)
> pl[-2] = 0;			                  // SS
> pl[-1] = 0;			                  // ind. rit.
> ```
> *Fonte: [Memoria Virtuale nel Nucleo.md](./Memoria%20Virtuale%20nel%20Nucleo)*

**Protezione e sicurezza:**
> Le interruzioni di protezione sono progetatte per poter **solamente _mantenere o alzare_** il _livello di privilegio_.
>
> In particolare, è bene che l'utente non possa modificare il valore salvato di `CS`.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

**Configurazione della IDT:**
> La `IDT` viene inizializzata tramite il programma di _bootstrap_, in particolare utilizzando l'istruzione `LIDTR` che carica l'indirizzo della `IDT` nel registro `IDTR` che il processore utilizza per accedere ala tabella e allocando `IDT` nella memoria `M1`.
> Per non permettere la modifica di `IDT` da parte dell'utente l'istruzione `LIDTR` è anch'essa **vietata** nel contesto _utente_.
>
> *Fonte: [Protezione.md](./Protezione#22-passaggi-tra-contesti)*

In sintesi, i gate della IDT sono meccanismi sofisticati che garantiscono transizioni sicure tra contesti, mentre il nuovo `%rsp` proveniva dal TSS (Task State Segment) che contiene la pila sistema dedicata a ciascun processo, assicurando che le informazioni critiche siano protette dalla corruzione da parte del codice utente.
Con l'introduzione della paginazione, la base della pila sistema si trova per ogni processo all'indirizzo virtuale `fin_sis_p`, e di conseguenza il nuovo valore di `%rsp` dopo il cambio pila sarà `fin_sis_p - 5 * sizeof(natq)`.

---

### Domanda 5.4
**Domanda:** Perché è necessario il cambio di pila negli attraversamenti di gate? Come funziona?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 5.5
**Domanda:** Come funziona l'istruzione `IRETQ`? Quali controlli di sicurezza effettua?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 5.6
**Domanda:** Perché si è resa necessaria l'introduzione della protezione nei sistemi di calcolo? Faccia riferimento all'evoluzione dai sistemi batch ai sistemi multiprogrammati.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 5.7
**Domanda:** Quali problemi sorgerebbero se due job diversi potessero accedere contemporaneamente alle stesse periferiche senza un sistema di protezione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 6. Paginazione e Memoria Virtuale

### Domanda 6.1
**Domanda:** Spiega il concetto di memoria virtuale e paginazione. Quali sono i vantaggi della suddivisione della memoria in pagine e frame?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.2 (answered)
**Domanda:** Cosa sono i registri LINF e LSUP? Quali problemi presentano nella gestione della memoria?

**Risposta:**
I registri LINF e LSUP sono una soluzione hardware per implementare la protezione della memoria nei sistemi multi-processo, precedente alla paginazione moderna.

**Funzione dei registri:**
> Per vietare all'`utente` l'accesso alle porzioni di `M2` di processi diversi, si inseriscono nella **CPU** _due registri_, `LINF` e `LSUP`, che hanno come compito quello **di contenere gli indirizzi dell'inizio e della fine della porzione di memoria virtuale del processo in esecuzione**.
> Entrambi i registri sono scrivibili _**solo da livello `sistema`**_.
>
> *Fonte: [Paginazione.md](./Paginazione#21-registri-limite-inferiore-e-superiore)*

**Meccanismo di protezione:**
> Quando la **CPU** lavora in modalità `utente`, deve controllare che gli accessi siano in indirizzi **_compresi_** nell'intervallo $\Bigl[$`LINF`, `LSUP`$\Bigr)$.
> In caso di accessi _out-of-bound_ si genererà invece un'eccezione di protezione `13`.

**Gestione durante il cambio processo:**
> Per permettere questa configurazione `LINF` non contiene più il primo indirizzo di `M2`, ma bensì quello della **prima locazione appartenente al processo in esecuzione**.
> Entrambi i registri devono quindi avere una posizione nel vettore `contesto` dei descrittori di processo, ovvero `I_LINF` e `I_LSUP`:
> - <u>Ogni volta</u> che un processo viene caricato dallo _swap_ il `sistema`, riferendosi alla parte di `M2` da lui occupata, **deve inizializzarne i campi**:
>   - `contesto[I_LINF]` con **l'indirizzo iniziale**
>   - `contesto[I_LSUP]` con **l'indirizzo finale**

**Traduzione degli indirizzi:**
> Da adesso in poi, la **CPU** interpreta ogni indirizzo `x` come `LINF + x`.
> In questo modo ogni _indirizzo "assoluto"_ di un processo, adesso indica semplicemente l'_offset_ da `LINF` di quel processo.

**Problemi principali:**

**1. Problema di rilocazione del codice:**
> Il primo risiede nel capire **dove salvare la sezione `.text` di ogni processo**. Infatti, a differenza della memoria unica, dove la sezione `.text` aveva un indirizzo costante dove essere salvata (`LINF`), nel caso di memorie multiple il _collegatore_, si troverà indirizzi di partenza variabili a seconda dello stato del sistema.

Le soluzioni includono:
- Compilazione indipendente dalla posizione (limitata a 32bit/2GB)
- Caricatore rilocante che modifica il programma all'avvio

**2. Problema degli indirizzi assoluti:**
> Un secondo problema risiede nel fatto che i processi potrebbero utilizzare **indirizzi assoluti** per salvare oggetti in memoria.

> Ipotizziamo di avere un processo `P1` che salva un indirizzo assoluto nella sua partizione. Questo processo viene poi **rimosso** e rilocato in un'altra, diversa dalla prima. A questo punto l'indirizzo assoluto che era utilizzato non sarà più disponibile, in quanto si riferisce ad una partizione adesso _out-of-bound_.

**Vincolo forte:**
> _Se un processo è locato in una determinata partizione della memoria, nel caso di scarica e carica, dovrà **sempre** essere rilocato nello stessa posizione._

**3. Frammentazione esterna:**
> Permane comunque il problema della _frammentazione esterna_.

**4. Memoria condivisa limitata:**
> Il terzo problema risiede sulla mancanza di una memoria condivisa, possibile con questo _hardware_, solamente tra processi in partizioni adiacenti.

**Superamento:**
Questi problemi hanno portato allo sviluppo della **paginazione**, che risolve la frammentazione esterna e permette maggiore flessibilità nella gestione della memoria virtuale, eliminando completamente la necessità dei registri LINF e LSUP.

---

### Domanda 6.3
**Domanda:** Qual è il problema della frammentazione esterna e come la paginazione lo risolve?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.4
**Domanda:** Perché è necessario che gli indirizzi Intel x86-64 siano normalizzati? Cosa significa "buco" nella memoria virtuale?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.5
**Domanda:** Cosa sono le tabelle di livello nel Trie-MMU? Come sono organizzate e indicizzate?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.6
**Domanda:** Evidenzia la differenza tra indirizzo virtuale e indirizzo fisico, ed esponi come funziona il table-walk nella traduzione degli indirizzi?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.7
**Domanda:** Qual è la differenza tra descrittori di pagina virtuale e descrittori di tabella?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.8
**Domanda:** Cos'è e com'è implementata la finestra sulla memoria fisica? Spieghi perché sono necessarie nel kernel?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.9
**Domanda:** Spiega il funzionamento del TLB (Translation Lookaside Buffer). Perché è necessario per le prestazioni?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.10
**Domanda:** Come viene gestito il bit D (dirty) nel TLB? Qual è il problema e la soluzione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.11
**Domanda:** Cosa sono le regioni e sottoregioni nella paginazione? Come si calcola la dimensione di una regione di livello i?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.12
**Domanda:** Descrivi le funzioni `map()` e `unmap()` per la gestione delle traduzioni. Come funzionano i parametri template e le espressioni lambda?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.13
**Domanda:** Come funziona l'iteratore `tab_iter`? Qual è la differenza tra visite anticipate e posticipate?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.14
**Domanda:** Spiega i bit di controllo nelle entrate delle tabelle: `P`, `R/W`, `U/S`, `PCD`, `PWT`, `A`, `D`.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.15
**Domanda:** Come vengono gestite le pagine di grandi dimensioni (`2MB`, `1GB`)? Cosa cambia nel `TLB`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.16
**Domanda:** Qual è la differenza tra `it.next()` e `it.down()` nell'iteratore `tab_iter`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.17
**Domanda:** Come è organizzata la memoria virtuale di un processo nel sistema studiato? Descriva la divisione tra sezioni sistema e utente, condivise e private.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.18
**Domanda:** Descriva il ciclo nelle tabelle di livello 4 e 3 della paginazione.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.19
**Domanda:** Cosa è il descrittore di frame e come viene utilizzato nella gestione della memoria?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 6.20
**Domanda:** Come vengono gestite le istruzioni `LOAD` e `STORE` nel contesto della memoria virtuale?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 7. Sistemi Multiprocesso e Processi

### Domanda 7.1 (answered)
**Domanda:** Cosa si intende per processo in un sistema multiprocesso? Quali sono le differenze tra programma e processo?

**Risposta:**
In un sistema multiprocesso, il concetto di **processo** rappresenta un'astrazione fondamentale che distingue nettamente l'entità statica del programma dalla sua esecuzione dinamica nel sistema.

**Definizione di Processo:**
> Un _**processo**_ è un programma in esecuzione su dei dati di ingresso
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

> Questa esecuzione la possiamo modellare come la sequenza degli stati attraverso il cui il sistema processore+memoria passa eseguendo il programma.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Differenze fondamentali tra Programma e Processo:**

**1. Natura statica vs dinamica:**
- **Programma**: Entità statica, sequenza di istruzioni memorizzate su disco
- **Processo**: Entità dinamica, rappresenta l'esecuzione attiva del programma

**2. Molteplicità delle relazioni:**
> L'importante è capire che _programma_ e _processo_ sono due cose <u>completamente distinte</u>, infatti:
> - Uno stesso programma può essere associato a **più processi**;
> - Uno stesso processo può eseguire, in sequenza, **più programmi**;
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Esempi pratici:**
- **Un programma → più processi**: Un editor di testo può essere eseguito in più finestre simultaneamente
- **Un processo → più programmi**: Un processo può chiamare `exec()` per eseguire programmi diversi in sequenza

**3. Influenza dell'ambiente:**
> In generale non è esclusivamente il programma a decidere attraverso quali stati il processo dovrà passare, ma hanno influenze anche i vari segnali di _input_
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**4. Ripetizione e cicli:**
> Il programma potrebbe contenere dei cicli, che scrivono le cose da ripetere una sola volta, mentre nel processo vediamo le azioni ripetute tante volte.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Caratteristiche distintive del processo:**

**Contesto di esecuzione:**
> Il _contesto di un processo_ comprenderà quindi:
> - Tutta la memoria (`M2`) usata dal processo. Qualora il processo non fosse in esecuzione, la immaginiamo per ora salvata nell'`HD`
> - Una **copia privata** di tutti i registri del processore, salvati in una opportuna struttura dati.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Significato contestuale:**
> In un sistema multiprocesso infatti, il significato di una istruzione _dipende dal processo che la sta eseguendo_.
> Se un processo `P1` esegue una istruzione `MOV %rax, 1000` si sta riferendo al "suo" registro `%rax` e al "suo" indirizzo `1000`.
> Mentre se la esegue un processo `P2` parlerà di un diverso `%rax` e di un diverso contenuto dell'indirizzo `1000`.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Stati di esecuzione del processo:**
Nel nostro sistema, ogni processo passa attraverso diversi stati durante la sua vita:

1. **Attivazione**: Creazione delle strutture dati necessarie
2. **Pronto**: Il processo è in attesa di essere selezionato per l'esecuzione
3. **Esecuzione**: Il processo ha il controllo della CPU
4. **Bloccato**: Il processo attende un evento (I/O, semaforo, ecc.)
5. **Terminazione**: Il processo ha completato la sua esecuzione

**Implementazione nel sistema:**
Dal file [`Sistemi Multiprocesso e Processi.md`](./Sistemi%20Multiprocesso%20e%20Processi):

```cpp
struct des_proc {
    natw id;                    // identificatore numerico del processo
    natw livello;              // livello di privilegio (LIV_UTENTE o LIV_SISTEMA)
    natl precedenza;           // precedenza nelle code dei processi
    vaddr punt_nucleo;         // indirizzo della base della pila sistema
    natq contesto[N_REG];      // copia dei registri generali del processore
    paddr cr3;                 // radice del TRIE del processo
    des_proc* puntatore;       // prossimo processo in coda
    void (*corpo)(natq);       // funzione da eseguire
    natq parametro;            // parametro della funzione
};
```

**Gestione del cambio di contesto:**
> Per cambiare il contesto quando passiamo da un processo all'altro (ovvero quando eseguiamo _swap-out_ e _swap-in_ della memoria) utilizziamo tecniche _sofware_:
> Ogni volta che si effettua un cambio di processo, andiamo a **salvare in una struttura dati** i valori dei registri e della memoria del processo terminato.
> Successivamente _copiamo i valori precedentemente salvati_ nella struttura dati associata al nuovo processo, rendendolo il **processo corrente**
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Controllo dell'esecuzione:**
> Il registro `RIP` del processore si trova **sempre** in uno e uno solo dei processi.
> `RIP` **non può** attraversare due processi diversi, se non tramite il cambio di processo effettuato dal `kernel` stesso.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Ruolo del kernel:**
> Il `kernel` è quindi un software che sta sempre nello spazio di memoria di sistema (`M1`) e può **riacquisire** occasionalmente il controllo del flusso.
> Il `kernel` gira infatti a **livello sistema** e può recuperare il controllo del flusso **<u>solamente</u>** tramite i _gate_ della `IDT` (interruzioni esterne, eccezioni, chiamate `int`)
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Riepilogo delle differenze chiave:**

| Aspetto           | Programma            | Processo                       |
| ----------------- | -------------------- | ------------------------------ |
| **Natura**        | Statico, su disco    | Dinamico, in esecuzione        |
| **Contesto**      | Nessuno              | Registri, memoria, stato       |
| **Ciclo di vita** | Permanente           | Nascita → esecuzione → morte   |
| **Interazione**   | Nessuna              | Input/output, segnali          |
| **Molteplicità**  | Uno → molti processi | Uno → molti programmi          |
| **Controllo**     | Passivo              | Attivo, controllato dal kernel |

In sintesi, mentre un **programma** è semplicemente una sequenza di istruzioni memorizzate, un **processo** rappresenta l'esecuzione attiva di quel programma con il proprio contesto, stato e ciclo di vita, gestito dinamicamente dal sistema operativo.

---

### Domanda 7.2
**Domanda:** Cosa comprende il contesto di un processo? Come viene gestito durante i cambi di processo?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.3
**Domanda:** Qual è il ruolo del kernel in un sistema multiprocesso? Come riacquisisce il controllo?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.4
**Domanda:** Descrivi gli stati di esecuzione di un processo: pronto, esecuzione, bloccato, terminazione.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.5
**Domanda:** Come funziona la schedulazione a priorità fissa? Quando è necessaria la schedulazione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.6
**Domanda:** Cosa significa _preemption_? Quando è necessaria nel nostro sistema?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.7
**Domanda:** Come avviene la transizione tra processi tramite i gate della `IDT`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.8
**Domanda:** Spiega la struttura del descrittore di processo (`des_proc`). Cosa contiene ogni campo?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.9
**Domanda:** Cosa contiene la pila sistema di un processo? Come vengono impostati `RIP`, `CS`, `RFLAGS`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.10
**Domanda:** Qual è il ruolo del processo `dummy`? Perché è necessario?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 7.11
**Domanda:** Descrivi l'organizzazione dei moduli sistema, io e utente. Come interagiscono tra loro?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 8. Realizzazione delle Primitive

### Domanda 8.1 (answered)
**Domanda:** Cos'è il contesto di un processo e come viene gestito il cambio di contesto in un sistema multiprogrammato?

**Risposta:**

**Definizione del contesto di un processo:**

Il contesto di un processo rappresenta tutto lo stato necessario per eseguire quel processo specifico. Nel nostro sistema comprende:

> Il _contesto di un processo_ comprenderà quindi:
> - La memoria `M2` usata dal processo.
> - Una **copia privata** di tutti i registri del processore, salvati in una opportuna struttura dati.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Struttura dati del descrittore di processo:**

Nel sistema, ogni processo ha un descrittore `des_proc` che contiene:

```cpp
struct des_proc {
    natw id;                    // identificatore numerico del processo
    natw livello;              // livello di privilegio (LIV_UTENTE o LIV_SISTEMA)
    natl precedenza;           // precedenza nelle code dei processi
    vaddr punt_nucleo;         // indirizzo della base della pila sistema
    natq contesto[N_REG];      // copia dei registri generali del processore
    paddr cr3;                 // radice del TRIE del processo
    des_proc* puntatore;       // prossimo processo in coda
    void (*corpo)(natq);       // funzione da eseguire
    natq parametro;            // parametro della funzione
};
```

*Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**Meccanismo del cambio di contesto:**

**1. Principio fondamentale:**

> **_L'unico modo per transizionare da un processo ad un altro è tramite un gate della `IDT`_**.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**2. Implementazione del cambio contesto:**

Il cambio contesto è realizzato tramite le funzioni `salva_stato` e `carica_stato`:

```x86asm
routine_gate:
    CALL salva_stato        ; Macro che salva il contenuto di tutti i registri in pila
    /*
    * corpo routine - qui può cambiare la variabile esecuzione
    */
    CALL carica_stato       ; Macro che carica il contenuto di tutti i registri dalla pila
    IRETQ
```

> Tutto il necessario per cambiare _processo_ è quindi **cambiare la variabile `esecuzione`** all'interno del _corpo della routine_.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**3. Gestione della variabile `esecuzione`:**

> Per capire a quale processo ci stiamo riferendo quando invochiamo `salva_stato` e `carica_stato` utilizziamo come già detto una variabile globale `esecuzione`.
> `esecuzione` è implementata come un **puntatore a descrittore di processo** `des_proc*`
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi)*

**4. Atomicità delle primitive:**

Per garantire la correttezza del sistema, le primitive devono essere atomiche:

> Per risolvere il problema delle _interferenze_ è sufficiente rendere **_atomiche_** le **primitive di sistema**.
> L'_atomicità_ implica che **tutta l'operazione deve essere eseguita interamente oppure per niente**, senza possibilità di essere interrotta.
>
> *Fonte: [Realizzazione Primitive.md](./Realizzazione%20Primitive)*

**Riepilogo del processo completo:**

1. Un gate della IDT viene attraversato (interruzione, eccezione, o `int`)
2. Il processore salva automaticamente 5 long word in pila (RIP, CS, RFLAGS, RSP, SS)
3. `salva_stato` salva tutti i registri nell'array `contesto` del processo corrente
4. Il corpo della routine può modificare la variabile `esecuzione`
5. `carica_stato` ripristina i registri dall'array `contesto` del nuovo processo
6. `IRETQ` ripristina le 5 long word e trasferisce il controllo al nuovo processo

Questo meccanismo garantisce che ogni processo mantenga il proprio contesto privato e che i cambi di contesto avvengano in modo atomico e sicuro.

---

### Domanda 8.2
**Domanda:** Descriva l'implementazione del cambio contesto. Cosa fanno le funzioni `salva_stato` e `carica_stato`? Come si relazionano con la variabile esecuzione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.3
**Domanda:** Come avviene la creazione dei processi nel sistema? Quali sono gli stati di esecuzione dei processi? A che punto del codice un processo si può dire effettivamente in esecuzione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.4
**Domanda:** Come funziona l'I/O nel sistema multiprogrammato? Quali sono gli accorgimenti da fare per mutex e sync?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.5
**Domanda:** Descriva l'avvio del sistema: quali strutture vengono create e come vengono inizializzate?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.6
**Domanda:** Come avviene il cambio di processo? Perché `punt_nucleo` punta alla base?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.7
**Domanda:** Come si passa il parametro alla `activate_p()`? Chi è che usa `punt_nucleo`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.8
**Domanda:** Perché usiamo una pila sistema diversa per ogni processo?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.9
**Domanda:** Come è organizzata la memoria virtuale di un processo nel sistema studiato?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 8.10
**Domanda:** Spieghi il problema dell'interferenza tra flussi di esecuzione. Faccia un esempio concreto di cosa potrebbe accadere durante l'inserimento di un processo nella coda "pronti", e esponga come viene risolto il problema degli stati inconsistenti nelle strutture dati del sistema

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 9. Semafori

### Domanda 9.1 (answered)
**Domanda:** Definisca i due problemi fondamentali della programmazione concorrente: mutua esclusione e sincronizzazione. Faccia esempi pratici per entrambi.

**Risposta:**
I due problemi fondamentali della programmazione concorrente nascono quando più processi o thread devono lavorare insieme condividendo risorse. Nel nostro sistema, dove i processi utente condividono memoria comune, questi problemi diventano critici.

**Contesto:**
> Per quanto riguarda la condivisione della memoria, nel nostro sistema i processi `utente`:
> - **Condividono**: le sezioni `.text`, `.data`, `.bus` del modulo `utente` e lo `heap`.
> - **Non condividono**: la _pila utente_
>
> *Fonte: [Semafori.md](./Semafori)*

> L'utente che scrive un'applicazione strutturata su più processi concorrenti deve affrontare dei problemi molto simili a quelli già affrontati a livello `sistema`.
> In particolare, anche l'utente deve affrontare il problema dell'_**interferenza**_
>
> *Fonte: [Semafori.md](./Semafori)*

**1. MUTUA ESCLUSIONE**

**Definizione:**
> **Mutua esclusione**: l'ordine nel quale eseguiamo le varie attività non è rilevante.
>
> *Fonte: [Semafori.md](./Semafori)*

**Esempio pratico fornito dal corso:**
> Un esempio pratico è la gestione del bagno durante un'esame.
> Non possono infatti esserci più studenti nello stesso momento in bagno, ed è necessario che uno torni poiché possa andare il prossimo.
> Tuttavia non è importante che uno studente vada prima di un altro.
>
> *Fonte: [Semafori.md](./Semafori)*

**Esempio informatico:**
Supponiamo di avere più processi che devono accedere a una struttura dati condivisa (es. un contatore globale):

```cpp
// PROBLEMA: senza mutua esclusione
int contatore_globale = 0;

void incrementa() {
    int temp = contatore_globale;    // Legge valore corrente
    temp = temp + 1;                // Incrementa localmente
    contatore_globale = temp;       // Scrive il nuovo valore
}
```

Se due processi eseguono `incrementa()` contemporaneamente, potrebbero leggere lo stesso valore, incrementarlo entrambi, e scrivere lo stesso risultato, perdendo un incremento.

**Soluzione con semafori:**
> Per risolvere questo problema è sufficente avere **un semaforo** che inizialmente contiene **un gettone** e imporre la regola che:
> _**"Solo chi ha il gettone può compiere una delle azioni. Al termine dell'azione è obbligatorio reinserire il gettone"**_
>
> *Fonte: [Semafori.md](./Semafori)*

Dal file [`Semafori.md`](./Semafori):

```cpp
natl mutex = sem_ini(1);

void incrementa_sicuro() {
    sem_wait(mutex);        // Prende il gettone (entra in sezione critica)
    int temp = contatore_globale;
    temp = temp + 1;
    contatore_globale = temp;
    sem_signal(mutex);      // Rilascia il gettone (esce da sezione critica)
}
```

**2. SINCRONIZZAZIONE**

**Definizione:**
> **Sincronizzazione**: alcune attività devono comunque essere eseguite prima di altre.
>
> *Fonte: [Semafori.md](./Semafori)*

**Esempio pratico fornito dal corso:**
> Un caso comune è quello in cui un processo produce dei dati e li scrive in un buffer intermedio, da cui in altro processo li preleva per svolgere ulteriori elaborazioni.
> In questo caso, finché il primo processo non ha prodotto i dati, il secondo non deve andare in esecuzione leggendoli.
> Allo stesso tempo chi scrive deve assicurarsi che l'altro ha letto correttamente tutti i dati, poiché li andrebbe a sovrascrivere.
>
> *Fonte: [Semafori.md](./Semafori)*

**Esempio dettagliato:**
Supponiamo due processi:
- **Produttore**: genera dati e li scrive in un buffer
- **Consumatore**: legge i dati dal buffer e li elabora

**Problema:** Il consumatore non deve leggere prima che il produttore abbia scritto, e il produttore non deve sovrascrivere dati non ancora letti.

**Soluzione con due semafori:**
> È sufficiente in questo caso utilizzare due _semafori_:
> - Una che eviti che `A` e `B` avvengano in contemporanea, inizializzata con `1 gettone`
> - Una che indichi che `A` è stata eseguita e che `B` ancora no, inizializzata _**vuota**_
>
> *Fonte: [Semafori.md](./Semafori)*

Dal file [`Semafori.md`](./Semafori):

```cpp
natl mutex = sem_ini(1);    // Mutua esclusione per accesso al buffer
natl sync = sem_ini(0);     // Sincronizzazione: inizia vuoto

void produttore() {
    sem_wait(mutex);        // Accesso esclusivo al buffer
    // ... scrivi dati nel buffer ...
    sem_signal(sync);       // Segnala che i dati sono pronti
    sem_signal(mutex);      // Rilascia l'accesso al buffer
}

void consumatore() {
    sem_wait(sync);         // Aspetta che i dati siano pronti
    sem_wait(mutex);        // Accesso esclusivo al buffer
    // ... leggi dati dal buffer ...
    sem_signal(mutex);      // Rilascia l'accesso al buffer
}
```

**Esempio avanzato - Handshake:**
> Nei casi di sincronizzazione, si può arrivare a sviluppare semafori che hanno funzionamento molto simile a quello degli _handshake_.
>
> *Fonte: [Semafori.md](./Semafori)*

Dal file [`Semafori.md`](./Semafori):

```cpp
natl S1 = sem_ini(1);   // Permesso di scrittura
natl S2 = sem_ini(0);   // Segnalazione lettura completata

void scrittura() {
    sem_wait(S1);       // Aspetta il permesso di scrivere
    // corpo: scrivi nel buffer
    sem_signal(S2);     // Segnala che la scrittura è completata
}

void lettura() {
    sem_wait(S2);       // Aspetta che ci sia qualcosa da leggere
    // corpo: leggi dal buffer
    sem_signal(S1);     // Segnala che la lettura è completata
}
```

**Differenze chiave:**
- **Mutua esclusione**: Impedisce accessi concorrenti alle risorse, l'ordine non importa
- **Sincronizzazione**: Coordina l'ordine di esecuzione delle operazioni, garantendo precedenze

**Implementazione nel sistema:**
I semafori nel nostro sistema sono implementati tramite:
- **Primitive di sistema**: `sem_ini()`, `sem_wait()`, `sem_signal()`
- **Struttura dati**: `des_sem` con contatore e coda di processi bloccati
- **Gestione automatica**: Processi bloccati quando il semaforo è vuoto, risveglio automatico quando diventa disponibile

Questi meccanismi sono fondamentali per costruire applicazioni concorrenti corrette e efficienti, risolvendo i problemi di interferenza che altrimenti comprometterebbero la correttezza dei programmi.

---

### Domanda 9.2
**Domanda:** Come funzionano i semafori di Dijkstra? Spieghi le operazioni di inserimento e prelievo dei gettoni e il comportamento in caso di semaforo vuoto.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 9.3
**Domanda:** Descriva cosa accade se il processo si sospende su un semaforo? Come viene gestita questa situazione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 10. Delay e Gestione del Tempo

### Domanda 10.1 (answered)
**Domanda:** Come viene implementata la primitiva `delay(n)` nel sistema? Perché non si decrementa semplicemente un contatore per ogni processo sospeso?

**Risposta:**

La primitiva `delay(n)` è implementata attraverso un sistema sofisticato di gestione dei processi sospesi che utilizza un timer hardware e una lista ordinata per ottimizzare le prestazioni. Il sistema evita di decrementare singoli contatori per motivi di efficienza computazionale.

**Implementazione del sistema timer:**

> Per realizzare questa funzionalità, il metodo più semplice è quello di impostare un _timer_ affinché venga inviata una richiesta di interruzione con **periodo fisso**. Questa è la soluzione che attuiamo nel nostro sistema, utilizzando il `timer 0` del _PC AT_, e programmandolo in modo che invii una richiesta ogni `50ms`.
>
> *Fonte: [Delay e new.md](./Delay%20e%20new.md)*

**Funzionamento della primitiva `delay(n)`:**

> Forniamo inoltre una _primitiva_ `void delay(natl n)` tramite la quale un processo può chiedere di essere sospeso per `n` **cicli del timer**. La primitiva inserisce il processo in una _coda di **processi sospesi**_, salvando il valore di `n`.
>
> *Fonte: [Delay e new.md](./Delay%20e%20new.md)*

**Problema del decremento di contatori singoli:**

> Se `k` fosse molto grande, andare a modificare tutti i singoli `ni` risulterebbe in un operazione molto costosa, poiché richiederebbe che il `driver` debba decrementare **tutti i `k` contatori** ad ogni interruzione del timer.
>
> *Fonte: [Delay e new.md](./Delay%20e%20new.md)*

**Soluzione ottimizzata - Lista ordinata con valori relativi:**

Dal file [`Delay e new.md`](./Delay%20e%20new.md):

```cpp
// Operativamente quello che facciamo è quindi diverso:
// - Manteniamo i processi in attesa in una lista ordinata per cicli di attesa crescenti
// - Per ogni processo non salviamo il numero di cicli totali che deve attendere,
//   ma quanti cicli in più rispetto al precedente

struct richiesta {
    natl d_attesa;      // tempo di attesa aggiuntivo rispetto alla richiesta precedente
    richiesta* p_rich;  // puntatore alla richiesta successiva
    des_proc* pp;       // descrittore del processo che ha effettuato la richiesta
};
```

> In altre parole, l'elemento in cima alla lista $r_1$ memorizza $n_1$, gli elementi $r_i$ con $(1 < i \le k)$ memorizzeranno: $n_i - n_{i-1}$.
>
> *Fonte: [Delay e new.md](./Delay%20e%20new.md)*

_**- Vantaggi del sistema di valori relativi**_

> In questo modo il `driver` deve occuparsi di decrementare **_solo l'elemento in testa alla lista_**. Quando questo elemento avrà il contatore a $0$, allora sposteremo i primi $k$ elementi con contatore nullo nella lista `pronti`, reinserendovi anche il processo in `esecuzione`, per poi chiamare lo `schedulatore()`.
>
> *Fonte: [Delay e new.md](./Delay%20e%20new.md)*

**Implementazione del driver del timer:**

Dal file [`Delay e new.md`](./Delay%20e%20new.md):

```cpp
extern "C" void c_driver_td(void) {
    inspronti();

    if (sospesi != nullptr) {
        sospesi->d_attesa--;    // Decrementa solo il primo elemento
    }

    while (sospesi != nullptr && sospesi->d_attesa == 0) {
        inserimento_lista(pronti, sospesi->pp);
        richiesta* p = sospesi;
        sospesi = sospesi->p_rich;
        delete p;
    }

    schedulatore();
}
```

**Approfondimenti:**
- Collegamento con interruzioni del timer (domanda 3.1 già answered): Il sistema utilizza le interruzioni hardware per garantire tempistiche precise
- Aspetti pratici: Riduzione della complessità da O(k) a O(1) per ogni tick del timer
- La gestione della lista ordinata con inserimento in posizione corretta mantiene l'efficienza del sistema anche con molti processi sospesi simultaneamente

---

### Domanda 10.2
**Domanda:** Spieghi l'algoritmo di gestione della lista ordinata dei processi sospesi. Come vengono calcolati i valori relativi invece che assoluti?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 10.3
**Domanda:** Come facciamo nel nostro nucleo a creare un modo per far sì che i processi partano e dopo un tot di tempo vadano in fondo alla coda pronti?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 11. Bus PCI

### Domanda 11.1 (answered)
**Domanda:** Quali problemi risolve lo standard PCI rispetto al bus ISA? Come viene evitato il problema degli indirizzi sovrapposti?

**Risposta:**

Lo standard PCI (Peripheral Component Interconnect) risolve diversi problemi critici del bus ISA, introducendo meccanismi innovativi per garantire compatibilità, espandibilità e gestione automatica delle risorse hardware.

**Problemi del bus ISA:**

> Dal momento che il `PC AT` era costruito con parti _standard_, il suo bus venne chiamato **ISA** (_Industry Standard Architecture_). Questo _bus_ era però molto limitato e descriveva tutti i limiti del `PC AT`.
>
> *Fonte: [PCI.md](./PCI.md)*

**Problema principale - Conflitti di indirizzi:**

> La maggior parte dei _computer_ dell'epoca (come oggi) erano **espandibili**, ovvero potevano essere inserite delle _schede di espansione_ indipendenti da _IBM_ per aggiungere funzionalità al sistema. Tuttavia questa possibilità generò dei problemi, specialmente in un mercato dove non era presente il coordinamento tra i vari produttori di schede. Poteva infatti capitare che due schede diverse facessero riferimento agli stessi indirizzi di memoria, e che quindi quegli indirizzamenti fossero **sovrapposti**.
>
> *Fonte: [PCI.md](./PCI.md)*

> Questo problema si propagava anche in come venivano scritti i _driver_, che non erano quindi più in grado di distinguere se la scheda desiderata era effettivamente inserita, se si trattava di una scheda diversa oppure se non ci fosse installato nulla.
>
> *Fonte: [PCI.md](./PCI.md)*

**Innovazioni dello standard PCI:**

**1. Universalità e indipendenza dal processore:**

> Nel 1992 la _Intel_ propose lo _standard_ **_Peripheral Component Interconnect_**, all'epoca a `32bit`, valido **per qualsiasi tipo di processore, non solo Intel**.
>
> *Fonte: [PCI.md](./PCI.md)*

**2. Tre spazi di indirizzamento:**

> Lo standard definisce inoltre **tre spazi di indirizzamento**: _spazio di memoria_, _spazio di I/O_ e _spazio di configurazione_. I primi due spazi sono completamente analoghi a quelli che già conosciamo e sono quelli che il _software_ **deve utilizzare per dialogare con le periferiche connesse al bus**.
>
> *Fonte: [PCI.md](./PCI.md)*

_**- Soluzione al problema degli indirizzi sovrapposti:**_

**Meccanismo dei comparatori programmabili:**

> Per garantire il vincolo che i _registri_ delle _periferiche_ non occupino indirizzi sovrapposti si introducono due regole:
> 1. Le _periferiche_ che rispettano lo standard **non possono scegliere autonomamente gli indirizzi dei propri registri**, ma devono contenere dei _**comparatori programmabili**_ in modo che questi indirizzi _siano impostati dal software della macchina_.
> 2. Il _software_ può impostare questi comparatori all'avvio del sistema, tramite il nuovo _spazio di configurazione_.
>
> *Fonte: [PCI.md](./PCI.md)*

**Spazio di configurazione:**

> Lo _spazio di configurazione_ è fatto in modo da poter accedere a dei **registri di configurazione** che ogni _periferica_ deve avere per rispettare lo _standard_ in modo **univoco** e **senza conflitti**. Tramite questi registri il software di avvio (`PCI BIOS`) può scoprire quali periferiche sono connesse al bus. Non solo, capisce anche di quanti indirizzi hanno bisogno e programma di conseguenza i comparatori affinché non ci siano _sovrapposizioni_.
>
> *Fonte: [PCI.md](./PCI.md)*

_**- Processo di configurazione automatica**_

Dal file [`PCI.md`](./PCI.md):

```cpp
// Registri BAR (Base Address Register) per ogni funzione:
// - Il software di inizializzazione scopre le dimensioni richieste
// - Assegna regioni non sovrapposte
// - Programma i comparatori delle periferiche
// - Abilita le funzioni tramite il registro Command

// Esempio di configurazione:
// 1. Lettura Vendor ID e Device ID per identificare la periferica
// 2. Lettura dei BAR per scoprire requisiti di spazio
// 3. Assegnazione indirizzi univoci
// 4. Attivazione tramite registro Command
```

**Meccanismo di indirizzamento nel bus:**

> **Fase di Indirizzamento**: dopo che l'_iniziatore_ specifica il tipo di operazione e l'indirizzo del primo byte da leggere/scrivere, **tutti i dispositivi che hanno registri nello spazio** confrontano queste informazioni con il contenuto dei loro comparatori. Al più uno troverà una corrispondenza, e quello diventerà l'_obiettivo_ della transazione.
>
> *Fonte: [PCI.md](./PCI.md)*

**Approfondimenti:**
- Collegamento con architettura moderna: Il PCI introduce il concetto di North Bridge e South Bridge per gestire diverse categorie di periferiche
- Aspetti pratici: La configurazione automatica elimina la necessità di jumper e switch manuali tipici del bus ISA
- Evoluzione: Il sistema PCI è alla base dei moderni standard PCIe mantenendo la compatibilità

**Vantaggi rispetto al bus ISA:**
1. **Configurazione automatica** vs configurazione manuale
2. **Assegnazione dinamica** degli indirizzi vs indirizzi fissi
3. **Rilevamento automatico** delle periferiche vs configurazione statica
4. **Eliminazione dei conflitti** attraverso software vs gestione manuale

---

### Domanda 11.2
**Domanda:** Cosa sono i tre spazi di indirizzamento definiti dallo standard PCI e qual è il ruolo dello spazio di configurazione?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 12. I/O e Driver

### Domanda 12.1 (answered)
**Domanda:** Perché in un sistema protetto un processo non può dialogare direttamente con le periferiche? Come vengono gestite le primitive di I/O?

**Risposta:**

In un sistema protetto, i processi utente non possono dialogare direttamente con le periferiche per garantire **sicurezza**, **stabilità** e **controllo centralizzato** dell'accesso alle risorse hardware.

**Motivazioni della protezione:**

**1. Problema delle istruzioni privilegiate:**

> Aver implementato la **Protezione** implica adesso che se un processo vuole fare un'operazione di `I/O`, **_non può parlare direttamente con le periferiche_** (tastiera, video, ...).
>
> *Fonte: [IO.md](./IO#2-driver-io)*

> Andremo quindi a **vietare le istruzioni di `IN`, `OUT`, `CLI`, `STI` per il _contesto `utente`_**, permettendole solamente quando ci si trova nel _contesto `sistema`_.
>
> *Fonte: [Protezione.md](./Protezione#2-protezione)*

**2. Controllo dell'accesso alle risorse:**

Le istruzioni `IN` e `OUT` sono fondamentali per comunicare con le periferiche, ma consentire l'accesso diretto comporterebbe rischi:

> Nei processori _Intel_ vi è un'associazione tra `IN` e `OUT` ai comandi `CLI` e `STI`. Se ponessimo il `LIV_UTENTE`, forniremmo l'accesso all'`utente` anche a queste istruzioni, cosa che abbiamo già visto non va fatta.
>
> *Fonte: [IO.md](./IO#3-modulo-io)*

**3. Problemi di sicurezza e interferenza:**

> Costringere l'utente 1 a scrivere il programma tramite _routine_ invece di dialogare direttamente con il controllore
> Costringere l'utente 2 a non disattivare le interruzioni
>
> *Fonte: [Protezione.md](./Protezione#2-protezione)*

**Gestione delle primitive di I/O:**

**1. Architettura del sistema:**

Il sistema è organizzato in **tre moduli separati** con livelli di privilegio differenti:

> Il sistema che realizzeremo è organizzato in tre moduli:
> - `sistema`
> - `io`
> - `utente`
>
> I moduli `sistema` e `utente` vengono eseguiti con il processore a **_livello sistema_**, in un contesto privilegiato, mentre `utente` verrà eseguito al _**livello utente**_.
>
> *Fonte: [Sistemi Multiprocesso e Processi.md](./Sistemi%20Multiprocesso%20e%20Processi#24-semplice-sistema-multiprocesso)*

**2. Primitive di I/O disponibili:**

> L'unica possibilità che gli abbiamo lasciato è quella di usare una _primitiva_, nella nostra macchina ad esempio abbiamo `readconsole()` e `writeconsole()`.
>
> *Fonte: [IO.md](./IO#2-driver-io)*

**Struttura delle primitive:**

Dal file [`IO.md`](./IO#2-driver-io):

```cpp
// Lettura di quanti byte in un buffer, nella periferica id
extern "C" void read_n(natl id, char* buf, natq quanti);

// Scrittura di quanti byte in un buffer, nella periferica id
extern "C" void write_n(natl id, const char* buf, natq quanti);
```

**3. Meccanismo di chiamata delle primitive:**

**Chiamata dall'utente:**

Dal file [`IO.md`](./IO#21-gestione-primitiva):

```x86asm
; Utente.s
.global read_n
read_n:
    int $IO_TIPO_RN  ; Chiamata di sistema tramite interruzione software
    ret
```

**Gestione nel sistema:**

```x86asm
; Sistema.s
.global a_read_n
.extern c_read_n
a_read_n:
    call c_read_n    ; Non salva/carica stato per efficienza
    iretq
```

Non effettuiamo `salva_stato`/`carica_stato` poiché faccaamo girare il driver nel contesto del processo attualmente in esecuzione.

**4. Implementazione delle primitive:**

**Gestione concorrenza e sincronizzazione:**

Dal file [`IO.md`](./IO#21-gestione-primitiva):

```cpp
extern "C" void c_read_n(natl id, natb *buf, natl quanti){
    des_io *d = &array_des_io[id];

    sem_wait(d->mutex);     // Garantisce mutua esclusione

    // Trasferisce informazioni al descrittore
    d->buf = buf;
    d->quanti = quanti;

    // Abilita le interruzioni sulla periferica
    outputb(1, d->iCTL);

    // Blocca il processo in attesa del completamento
    sem_wait(d->sync);

    sem_signal(d->mutex);   // Rilascia mutua esclusione
}
```

**5. Gestione attraverso driver e processi esterni:**

**Architettura driver:**

> L'operazione nel suo complesso ha quindi due attori:
> - La _primitiva_ ha lo scopo di avviare l'operazione di `I/O` e bloccare il processo, garantendo la _mutua esclusione_
> - Il _driver_ ha il compito di trasferire effettivamente i byte e sbloccare il processo quando l'operazione si è conclusa.
>
> *Fonte: [IO.md](./IO#2-driver-io)*

**Modulo I/O separato:**

> Per risolvere invece il secondo punto "trasformiamo" il _driver_ in un _processo_ in un nuovo modulo, chiamato `modulo I/O`.
> Il `modulo I/O` è un modulo indipendente, così come `sistema` e `utente`.
>
> *Fonte: [IO.md](./IO#3-modulo-io)*

**6. Protezione e verifica dati utente:**

**Principio di non fiducia:**

> Lo standard che assumiamo è quello di **_non fidarci dell'utente_**.
> Sarà quindi necessario controllare e approvare i dati che l'utente ci fornisce, in particolare il _buffer_ dove salvare i dati, che ci è restituito attraverso un'indirizzo.
>
> *Fonte: [IO.md](./IO#23-verifica-dati-utente)*

**Controlli di sicurezza:**
1. Verifica che l'indirizzo sia normalizzato
2. Verifica che l'indirizzo sia mappato nel processo
3. Controllo accessi in scrittura per i buffer
4. Prevenzione wrap-around degli indirizzi
5. Verifica che tutto sia nella memoria condivisa

**Vantaggi dell'approccio protetto:**

1. **Sicurezza**: Prevenzione di accessi non autorizzati alle periferiche
2. **Stabilità**: Evita che processi utente corrompano il sistema
3. **Controllo centralizzato**: Gestione coordinata delle risorse hardware
4. **Isolamento**: Separazione tra processi e sistema operativo
5. **Robustezza**: Gestione degli errori controllata dal kernel

**Flusso completo di una operazione I/O:**

1. Processo utente chiama primitiva tramite `INT`
2. Sistema passa a livello privilegiato
3. Primitiva valida parametri e attiva periferica
4. Processo viene bloccato su semaforo
5. Interruzione hardware segnala completamento
6. Driver processa i dati e risveglia processo
7. Controllo torna al processo utente

Questo meccanismo garantisce che l'accesso alle periferiche avvenga sempre sotto il controllo del sistema operativo, mantenendo la sicurezza e l'integrità del sistema.

---


### Domanda 12.2
**Domanda:** Descriva l'I/O con primitiva di sistema. Come funziona la primitiva di lettura da interfaccia?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.3
**Domanda:** Come funziona il driver e chi lo chiama nel sistema di I/O?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.4
**Domanda:** Com'è collegato il modulo I/O con il resto del sistema?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.5
**Domanda:** Come funziona la `activate_pe()`? Cosa fa la `wfi()`?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.6
**Domanda:** Quali sono le differenze tra primitiva di sistema e driver, primitiva di I/O e handler?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.7
**Domanda:** Come viene gestita la differenza tra Bus Mastering e le primitive di I/O tradizionali?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.8
**Domanda:** Descriva il processo di configurazione e inizializzazione del sistema I/O all'avvio.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.9
**Domanda:** Come viene gestita la sincronizzazione tra processi e operazioni di I/O?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.10
**Domanda:** Quali sono i vantaggi e gli svantaggi dell'approccio con driver separati?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 12.11
**Domanda:** Come viene garantita la protezione nell'accesso alle periferiche tramite il sistema di I/O?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 13. DMA (Direct Memory Access)

### Domanda 13.1 (answered)
**Domanda:** Cos'è il DMA e quali vantaggi offre rispetto al controllo di programma e alle interruzioni?

**Risposta:**

Il **DMA (Direct Memory Access)** è una modalità di trasferimento dati che permette ai dispositivi di scambiare informazioni direttamente con la memoria RAM **senza coinvolgere la CPU**, eliminando l'overhead delle modalità tradizionali.

**Definizione e Meccanismo:**

Dal file [`DMA.md`](./DMA.md#2-dma):

> La modalità `DMA` (_Direct Memory Access_) prevede invece che sia direttamente il dispositivo ad eseguire le operazioni di lettura o scrittura necessarie sulla **RAM**, **_senza coinvolgere la CPU_**.

Il dispositivo DMA deve essere dotato di:
- Un **sommatore** per calcolare autonomamente gli indirizzi a partire dall'indirizzo base `b`
- Un **contatore** che decrementi il numero di byte `n` ad ogni trasferimento completato
- **Registri di controllo** per gestire stato e configurazione

**Confronto con Modalità Tradizionali:**

*Dal file [`DMA.md`](./DMA.md#2-dma):*

**1. Controllo di Programma:**
> **A "controllo di programma"**: più veloce per quanto riguarda il trasferimento stesso, ma blocca la **CPU** per tutta la durata temporale tra il primo trasferimento e l'ultimo

**2. Interruzioni:**
> **Tramite Interruzioni**: più lento, ma permette di utilizzare la **CPU** per eseguire altri _processi_ durante le attese tra un trasferimento e l'altro

**3. DMA:**
> Per entrambe le modalità è previsto il coinvolgimento della **CPU**, che dovrà eseguire prima una _lettura_ e poi una scrittura, comportando due scambi dati sul `bus`

**Protocollo HOLD/HOLDA ("Cycle Stealing"):**

Dal file [`DMA.md`](./DMA.md#2-dma):

Il coordinamento tra CPU e DMA avviene tramite handshake:

1. **Richiesta**: Il dispositivo attiva `HOLD` quando vuole trasferire
2. **Cessione**: La CPU termina l'operazione corrente, mette i piedini in alta impedenza e attiva `HOLDA`
3. **Trasferimento**: Il dispositivo attiva i suoi piedini ed esegue il trasferimento
4. **Rilascio**: Il dispositivo disattiva `HOLD` e rimette le uscite in alta impedenza
5. **Ripresa**: La CPU disattiva `HOLDA` e riprende il normale funzionamento

> In pratica, la **CPU** si mette in attesa dando la precedenza al `DMA` nell'accesso al bus. Questa tecnica è chiamata _"cycle stealing"_, in quanto il `DMA` "ruba" cicli di bus alla **CPU**.

**Vantaggi del DMA:**

Dal file [`DMA.md`](./DMA.md#2-dma):

Il meccanismo resta vantaggioso in tre scenari principali:

> 1. Se la **CPU** è più lenta della **RAM** (storico - home computer anni '80)
> 2. Se il trasferimento a controllo di programma non è abbastanza veloce per il dispositivo
> 3. Se il dispositivo deve trasferire i dati con più urgenza di quanto permesso dal meccanismo delle interruzioni

**Esempio Moderno:**
> Gli ultimi due scenari possono verificarsi anche oggi, basta pensare ad alcune _schede di rete_ che possono ricevere o inviare decine di milioni di pacchetti al secondo a velocità di `200 Gbps`.

**Vantaggi con Cache:**

Dal file [`DMA.md`](./DMA.md#21-interazione-con-la-cache):

> Se inseriamo la _cache_ [...] viene introdotto un grande vantaggio: La **CPU**, statisticamente, può ora eseguire più istruzioni. Infatti parte delle istruzioni in memoria saranno probabilmente salvate proprio in _cache_ e non richiederanno un accesso alla **RAM**.

**Riepilogo Comparativo:**

| **Modalità**            | **Coinvolgimento CPU** | **Velocità**            | **Overhead**   | **Adatto per**          |
| ----------------------- | ---------------------- | ----------------------- | -------------- | ----------------------- |
| **Controllo Programma** | Completo               | Alto per singolo trasf. | Blocca CPU     | Piccoli trasferimenti   |
| **Interruzioni**        | Per ogni byte          | Basso                   | Context switch | Trasferimenti sporadici |
| **DMA**                 | Solo setup iniziale    | Massimo                 | Minimo         | Grandi volumi di dati   |

Il DMA rappresenta quindi la **soluzione ottimale per trasferimenti di grandi quantità di dati**, liberando la CPU per altre attività e massimizzando l'efficienza del sistema.

---

### Domanda 13.2
**Domanda:** Cosa sono lo snooping e lo snarfing nella gestione della cache con DMA?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.3
**Domanda:** Come interagisce il DMA con la MMU? Quali problemi sorgono con la memoria virtuale?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.4
**Domanda:** Cos'è il PCI Bus Mastering? Come funziona l'arbitraggio tra più dispositivi?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.5
**Domanda:** Cosa sono i PRD (Physical Region Descriptors)? Come gestiscono buffer discontigui?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.6
**Domanda:** Quali sono i registri del Bus Master IDE Controller e come vengono programmati?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.7
**Domanda:** Qual è il problema dei confini di 64KiB nel DMA? Come si risolve?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.8
**Domanda:** Come funziona la bufferizzazione nel ponte PCI e quali problemi crea con le interruzioni?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*


---

### Domanda 13.9
**Domanda:** Come funziona il protocollo HOLD/HOLDA per l'arbitraggio dell'accesso alla RAM in modalità DMA? Descriva i passi della comunicazione.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 13.10 (answered)
**Domanda:** Spieghi l'interazione tra DMA e cache. Quali problemi sorgono con le politiche `write-through` e `write-back` e come vengono risolti nei processori Intel e ARM?

**Risposta:**
L'interazione tra DMA e cache presenta sfide significative per la coerenza dei dati, poiché il DMA accede direttamente alla RAM bypassando la cache della CPU.

**Problemi principali:**
> Questi problemi nascono dal fatto che le operazioni del `DMA` potrebbero coinvolgere parti di **RAM** che erano state _**precedentemente copiate in cache**_.
>
> *Fonte: [DMA.md](./DMA#21-interazione-con-la-cache)*

**Politica Write-Through:**
Con la politica `write-through`, dal file [`DMA.md`](./DMA#211-politica-write-through):

*Soluzione Hardware (Intel):*
> Nei processori _Intel_ la soluzione è risolta in `hardware`. Si fa in modo che il _controllore cache_ **_osservi tutte le possibili sorgenti di scritture in RAM_** attraverso il bus condiviso, processo chiamato di _snooping_.

> Se le linee di controllo identificano un operazione di _scrittura_, il controllore può usare il contenuto delle linee di indirizzo per eseguire una normale ricerca in _cache_, e **nel caso di `hit` invalidare in autonomia la corrispondente `cacheline`**.

*Soluzione Software (ARM):*
> Nei sistemi `ARM` il problema è invece delegato al _software_, tramite istruzioni dedicate che permettono alla **CPU** di interagire direttamente con il _controllore cache_ e invalidarne le `cacheline`. Il _software_ dovrà quindi eseguire tutte le istruzioni specificando l'intervallo `[b, b+n)` (allineato opportunamente alle _cacheline_) **_subito dopo che il trasferimento sia terminato_**.

**Politica Write-Back:**
> In questa politica le scritture della **CPU** vengono mantenute soltanto in _cache_ e effettuate in maniera _sincrona_ in secondi momenti. Questa politica comporta un problema sia nelle operazioni di _uscita_ su `DMA`, poiché il buffer di lettura in **RAM** **potrebbe contenere memoria non aggiornata**.
>
> *Fonte: [DMA.md](./DMA#21-interazione-con-la-cache)*

**Esempio pratico - Gestione letture DMA:**
> Per risolvere possiamo utilizzare la tecinca di _snooping_, ma in questo caso il _controllore cache_ **_deve implementare lo snarfing per le cacheline dirty_**.
>
> *Fonte: [DMA.md](./DMA#21-interazione-con-la-cache)*

**Differenze principali:**
- **Intel**: Risoluzione hardware automatica tramite snooping
- **ARM**: Risoluzione software con istruzioni dedicate per invalidazione cache
- **Write-through**: Problemi minori, risolti con invalidazione
- **Write-back**: Problemi più complessi, richiedono gestione delle cacheline dirty

---

---

### Domanda 13.11
**Domanda:** Descriva il meccanismo di PCI Bus Mastering e l'interazione con le interruzioni. Come viene risolto il problema della bufferizzazione nel ponte PCI?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## 14. Architettura Moderna CPU

### Domanda 14.1 (answered)
**Domanda:** Descriva le dipendenze sui dati e sui nomi nell'esecuzione delle istruzioni. Come vengono gestite?

**Risposta:**
Le dipendenze nell'esecuzione delle istruzioni sono vincoli che impediscono di riordinare liberamente le istruzioni per mantenere la correttezza semantica del programma. Nell'architettura moderna delle CPU sono fondamentali per l'esecuzione fuori ordine.

**Classificazione delle dipendenze:**
> Affinché il risultato finale dei registri sia significativo, dobbiamo rispettare una serie di condizioni chiamate **_Dipendenze_**, che si dividono in tre tipi:
> - **_Dipendenze sui Dati_**
> - **_Dipendenze sui Nomi_**
> - **_Dipendenze sui Controllo_**
>
> Le **_dipendenze_** sono **proprietà del programma**, _indipendenti dal circuito che esegue il programma_.
>
> *Fonte: [Architettura Moderna CPU Intel.md](./Architettura%20Moderna%20CPU%20Intel#33-dipendenze)*

**1. Dipendenze sui Dati (Data Dependencies)**

**Definizione:**
> Un'istruzione `i` **dipende dai dati** di un'altra istruzione `j`, _precedente ad essa_, se `i` utilizza come uno dei registri `src` il registro `dst` di `j`.

**Esempio:**
```x86asm
ADD R1, R2, R3
; ...
SUB R3, R4, R5  ; R3 dipende dal risultato della ADD
```

**Caratteristiche:**
> Le **_Dipendenze sui Dati_** forzano le istruzioni dipendenti a _non poter essere riordinate liberamente_, poiché è necessario che l'istruzione `i` venga eseguita <u>dopo</u> `j`, per avere il contenuto corretto del registro che dovrà utilizzare.

**2. Dipendenze sui Nomi (Name Dependencies)**

Le dipendenze sui nomi si dividono in due categorie:

**A) Antidipendenze (WAR - Write After Read):**
> Un'istruizone `i` si dice _**antidipendente**_ da un'altra istruzione `j`, _successiva ad essa_, se `i` utilizza come `src` lo stesso registro `dst` di `j`.

**Esempio:**
```x86asm
ADD R1, R2, R3    ; Legge R1
; ...
SUB R4, R5, R1    ; Scrive R1 - antidipendenza
```

**B) Dipendenze in uscita (WAW - Write After Write):**
> Un'istruzione `i` si dice **_dipendente in uscita_** rispetto ad un'altra `j`, se entrambe vogliono scrivere nello stesso registro `dst`.

**Esempio:**
```x86asm
ADD R1, R2, R3    ; Scrive R3
; ...
SUB R4, R5, R3    ; Scrive R3 - dipendenza in uscita
```

**Gestione delle dipendenze:**
Dal file [`Architettura Moderna CPU Intel.md`](./Architettura%20Moderna%20CPU%20Intel#331-dipendenze-sui-dati):

**1. Gestione dipendenze sui dati:**
> Per risolvere questo tipo di dipendenze, facciamo in modo che l'`emissione` setti il bit `W` del registro `dst` dell'istruzione che sta emettendo.
>
> La stazione di `emissione`, prima di inviare i dati alla `ALU`, valuterà il bit `W` dei sorgenti, inviandoli **solamente quando `dst->W == 0`**.

**2. Gestione dipendenze sui nomi:**
> Un modo per risolvere le **dipendenze sui nomi** è far andare in **stallo l'`emissione`**. Questa può infatti entrare e uscire liberamente dallo stato di stallo, tramite dei controlli.

**Controlli specifici:**
- **Dipendenze in uscita**: Si controlla il bit `dst->W`, propagando l'istruzione solo se `dst->W == 0`
- **Antidipendenze**: Si valuta il campo `dst->C`, propagando solo se `dst->C == 0`

**3. Rinomina dei registri (Register Renaming):**
> Per quanto riguarda le **dipendenze sui nomi**, esse sono anche chiamate **dipendenze fittizie**, questo perché se andiamo a sovrascrivere il contenuto di un registro è perché adesso lo vogliamo utilizzare per fare altro.
>
> Possiamo quindi risolverle in questo modo:
> - _Antidipendenze_: è sufficiente cambiare il registro `dst` di `j`.
> - _Dipendenze in uscita_: cerchiamo per ogni scrittura un registro non utilizzato da nessun'altro.

**Implementazione con registri logici e fisici:**
> Un modo per semplificare questo passaggio è inserire, prima degli $n$ registri fisici `Fi`, una tabella contentente $m$ **_registri logici_** `Ri` che punteranno ai registri fisici non in uso.
>
> In questa nuova architettura, le istruzioni tradotte dalla `decode` faranno riferimento ai **registri logici**.
>
> L'`emissione` si preoccuperà quindi anche di tradurli in _**registri fisici**_ `Fi` con l'accortezza che:
> Il `dst` di una operazione deve **sempre** essere un registro fisico _attualmente non puntato da nessun altro e non utilizzato dalla `ALU`_ (`W == 0 && Count == 0`).

**Vantaggi del sistema:**
La gestione efficace delle dipendenze permette:
- **Esecuzione fuori ordine**: Le istruzioni possono essere eseguite non appena i loro operandi sono disponibili
- **Esecuzione speculativa**: Le istruzioni possono essere eseguite prima di sapere se dovranno essere effettivamente completate
- **Parallelismo**: Più istruzioni indipendenti possono essere eseguite contemporaneamente

Questo sistema è alla base delle moderne CPU Intel e permette di ottenere prestazioni elevate mantenendo la correttezza semantica dei programmi.

---

### Domanda 14.2
**Domanda:** Come si risolvono le dipendenze sul controllo nell'architettura moderna?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.3
**Domanda:** Spieghi l'esecuzione speculativa e out of order con diagrammi. Come funziona la pipeline?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.4
**Domanda:** Cosa è il `ROB` (Reorder Buffer) e a cosa serve nell'esecuzione out-of-order?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.5
**Domanda:** Qual è la differenza tra registri logici e fisici nell'architettura moderna?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.6
**Domanda:** Parli delle pipeline nell'architettura moderna e dei loro vantaggi.

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.7
**Domanda:** Come vengono gestite le istruzioni `LOAD` e `STORE` nell'esecuzione speculativa?

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---

### Domanda 14.8
**Domanda:** Come funziona l'interazione tra esecuzione speculativa e cache nelle CPU moderne? Descriva i problemi di sicurezza legati alla speculazione (Meltdown, Spectre).

**Risposta:**
*[La risposta verrà aggiunta quando richiesta]*

---
<div class="stop"></div>

---

## Note per lo Studio

- Le risposte verranno aggiunte progressivamente durante le sessioni di studio
- Ogni risposta includerà riferimenti specifici ai file sorgente
- Gli esempi pratici aiuteranno a consolidare i concetti teorici
- Si consiglia di rispondere prima autonomamente e poi confrontare con la risposta ufficiale
